# xhs_user_spider_ai.py (v5.5 - å…³é”®ä¿®å¤ç‰ˆ)
# - ã€åŸºå‡†ã€‘æœ¬ä»£ç ä¸¥æ ¼åŸºäºæ‚¨æä¾›çš„ v5.4 ç‰ˆæœ¬è¿›è¡Œä¿®æ”¹ã€‚
# - ã€æ ¸å¿ƒä¿®å¤ Iã€‘ä¿®æ­£äº† /mark_data è·¯ç”±çš„å»é‡é€»è¾‘ã€‚ç°åœ¨åªæœ‰çŠ¶æ€ä¸ºâ€œç¬¦åˆâ€æˆ–â€œä¸ç¬¦åˆâ€çš„ç”¨æˆ·æ‰ä¼šè¢«æ·»åŠ åˆ°Rediså»é‡é›†åˆä¸­ã€‚è¿™å½»åº•è§£å†³äº†ä¹‹å‰å·²å­˜åœ¨ï¼ˆå¦‚â€œä¸ç¬¦åˆâ€ï¼‰çš„ç”¨æˆ·æ— æ³•è¢«å†æ¬¡æäº¤ä¸ºâ€œäººå·¥å®¡æ ¸â€çŠ¶æ€çš„BUGã€‚
# - ã€æ ¸å¿ƒä¿®å¤ IIã€‘ä¿®æ­£äº† /get_review_list è·¯ç”±çš„åŠŸèƒ½ã€‚é‡‡ç”¨æ›´ç¨³å¥çš„â€œåˆ é™¤è¡Œâ€æ–¹å¼æ¥æ¸…ç©ºè¡¨æ ¼ï¼Œè€Œä¸æ˜¯â€œåˆ›å»ºæ–°æ–‡ä»¶â€ï¼Œé¿å…äº†æ½œåœ¨çš„æ–‡ä»¶æƒé™å’Œæ•°æ®ä¸¢å¤±é—®é¢˜ï¼Œç¡®ä¿â€œå¤å®¡â€æŒ‰é’®èƒ½æ­£å¸¸å·¥ä½œã€‚
# - ã€ä¿ç•™æ¶æ„ã€‘ä¿ç•™äº† v5.4 ç‰ˆæœ¬çš„æ‰€æœ‰ä¼˜è‰¯ç‰¹æ€§ï¼ŒåŒ…æ‹¬çº¿ç¨‹å®‰å…¨çš„Rediså†™å…¥ã€åå°ä»»åŠ¡é˜Ÿåˆ—ã€å››é€šé“æ•°æ®å­˜å‚¨ç­‰ã€‚

# -*- coding: utf-8 -*-
import os
import io
import json
import queue
import atexit
import signal
import hashlib
import logging
import threading
import re
import platform
import subprocess
import time
from datetime import datetime, timedelta
from typing import Optional, List, Dict

from concurrent.futures import ThreadPoolExecutor

import requests
import pandas as pd
from PIL import Image
import redis
from flask import Flask, request, jsonify, Response, send_file, render_template_string
from flask_cors import CORS
from werkzeug.serving import WSGIRequestHandler
from filelock import FileLock, Timeout

import torch
import joblib
import numpy as np
import torch.nn as nn
from sentence_transformers import SentenceTransformer
from openpyxl import Workbook, load_workbook

# ========== åŸºç¡€é…ç½® ==========
FLASK_PORT = 5001
APPROVED_EXCEL_PATH = "å·²é€šè¿‡æ•°æ®.xlsx"
MANUAL_REVIEW_EXCEL_PATH = "å¾…å¤å®¡æ•°æ®.xlsx"
NEW_TRAINING_DATA_EXCEL = "å¾…è®­ç»ƒæ•°æ®.xlsx"
DELTA_PREFIX = "å·¥ä½œæˆæœ_"
FETCHED_USER_LIST_PATH = "å·²çˆ¬å–ç”¨æˆ·å.xlsx" 
IMAGES_ROOT = os.path.join("data", "images")
IMAGE_MAX_SIDE = 768; IMAGE_FORMAT = "WEBP"; IMAGE_QUALITY = 90
REDIS_HOST = "localhost"; REDIS_PORT = 6379; REDIS_DB = 0
BATCH_FLUSH_ROWS = 20; BATCH_FLUSH_SEC  = 10.0; MAX_QUEUE_SIZE = 2000
IMG_MAX_WORKERS  = 6; HTTP_TIMEOUT = (3, 8)
WAL_DIR   = os.path.join("data", "wal_final"); WAL_FILE  = os.path.join(WAL_DIR, "mark_data.jsonl")

# ========== Redis Keys ==========
AI_ENABLED_KEY = "ai:enabled"
SAVE_HISTORY_ENABLED_KEY = "save_history:enabled"
WAL_DONE_SET = "wal:done_final"
USERNAMES_SET_KEY = "usernames_set"
USERIDS_SET_KEY = "userids_set"
CUR_APPROVED = "export_cursor:approved"
CUR_REJECTED = "export_cursor:rejected"

# ========== é” & çº¿ç¨‹å®‰å…¨ ==========
mark_data_lock = threading.Lock() 

# ========== é‚®ç®±æå–åŠŸèƒ½ ==========
EMAIL_MAPPING_DICT = { "è‰¾ç‰¹": "@", " at ": "@", " at": "@", "at ": "@", "Â©": "@", "Â®": "@", "ğŸ¥": "@", "ï¼ ": "@", "(at)": "@", "[at]": "@", "At": "@", "A T": "@", "ğŸ§": "qq", "qqå·": "qq", "æ‰£æ‰£": "qq", "æ‰£": "q", "q ": "q", "ä¼é¹…": "qq", "çƒçƒ": "qq", "163": "163", "ä¸€å…­ä¸‰": "163", "ä¸€äºŒå…­": "126", "126": "126", "æ–°æµª": "sina", "è°·æ­Œ": "gmail", "outlook": "outlook", "hotmail": "hotmail", "ç‚¹": ".", "dian": ".", " dot ": ".", " dot": ".", "dot ": ".", "ã€‚": ".", "Â·": ".", "ä¸¶": ".", " . ": ".", "com": "com", "åº·å§†": "com", "ç‚¹com": ".com", "cn": "cn", "ç‚¹cn": ".cn", "é›¶": "0", "ä¸€": "1", "äºŒ": "2", "ä¸‰": "3", "å››": "4", "äº”": "5", "å…­": "6", "ä¸ƒ": "7", "å…«": "8", "ä¹": "9", }
def extract_and_normalize_email(text: str) -> Optional[str]:
    if not text: return None
    normalized_text = text.lower()
    for non_standard, standard in EMAIL_MAPPING_DICT.items(): normalized_text = normalized_text.replace(non_standard, standard)
    normalized_text = re.sub(r'[\s:ï¼š,ï¼Œ\(\)\[\]]', '', normalized_text)
    match = re.search(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}', normalized_text)
    if match: return match.group(0)
    qq_match = re.search(r'([1-9][0-9]{5,10})qq\.com', normalized_text)
    if qq_match: return f"{qq_match.group(1)}@qq.com"
    return None

# ========== AI æ¨¡å‹åŠ è½½ ==========
MODEL_PATH = 'blogger_classifier_model.pth'
SCALER_PATH = 'scaler.joblib'
TEXT_MODEL_NAME = r'F:\AI_Model_Project\huggingface_cache\moka-ai_m3e-base'
IMAGE_MODEL_NAME = 'sentence-transformers/clip-ViT-B-32'
NUM_NOTES_TO_PROCESS = 20
AI_REJECT_THRESHOLD = 0.3
AI_ACCEPT_THRESHOLD = 0.55
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
os.environ['HF_HOME'] = './huggingface_cache'
print("ğŸ¤– æ­£åœ¨åˆå§‹åŒ–æœ¬åœ°AIç¯å¢ƒ...")
AI_ENABLED_BY_FILE = False
try:
    if not os.path.exists(MODEL_PATH) or not os.path.exists(SCALER_PATH): raise FileNotFoundError(f"æ¨¡å‹({MODEL_PATH})æˆ–æ ‡å‡†åŒ–æ–‡ä»¶({SCALER_PATH})ä¸å­˜åœ¨ã€‚")
    AI_DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"   - ä½¿ç”¨è®¾å¤‡: {AI_DEVICE.upper()}")
    print(f"   - æ­£åœ¨åŠ è½½åˆ†ç±»å™¨æ¨¡å‹: {MODEL_PATH}")
    model_state_dict = torch.load(MODEL_PATH, map_location=AI_DEVICE, weights_only=True)
    input_dim = model_state_dict['network.0.weight'].shape[1]
    class BloggerClassifier(nn.Module):
        def __init__(self, input_features):
            super(BloggerClassifier, self).__init__(); self.network = nn.Sequential(nn.Linear(input_features, 512), nn.BatchNorm1d(512), nn.ReLU(), nn.Dropout(0.5), nn.Linear(512, 256), nn.BatchNorm1d(256), nn.ReLU(), nn.Dropout(0.5), nn.Linear(256, 128), nn.BatchNorm1d(128), nn.ReLU(), nn.Dropout(0.4), nn.Linear(128, 1), nn.Sigmoid())
        def forward(self, x): return self.network(x)
    AI_MODEL = BloggerClassifier(input_features=input_dim); AI_MODEL.load_state_dict(model_state_dict); AI_MODEL.to(AI_DEVICE); AI_MODEL.eval()
    print("   - åˆ†ç±»å™¨åŠ è½½æˆåŠŸ!")
    print(f"   - æ­£åœ¨åŠ è½½Scaler: {SCALER_PATH}"); AI_SCALER = joblib.load(SCALER_PATH); print("   - ScaleråŠ è½½æˆåŠŸ!")
    print(f"   - æ­£åœ¨åŠ è½½æ–‡æœ¬æ¨¡å‹: {TEXT_MODEL_NAME} ..."); TEXT_EMBEDDING_MODEL = SentenceTransformer(TEXT_MODEL_NAME, device=AI_DEVICE); print("   - æ–‡æœ¬æ¨¡å‹åŠ è½½æˆåŠŸ!")
    print(f"   - æ­£åœ¨åŠ è½½å›¾åƒæ¨¡å‹: {IMAGE_MODEL_NAME} ..."); IMAGE_EMBEDDING_MODEL = SentenceTransformer(IMAGE_MODEL_NAME, device=AI_DEVICE); print("   - å›¾åƒæ¨¡å‹åŠ è½½æˆåŠŸ!")
    print("\nâœ… AIæ¨¡å‹å…¨éƒ¨åŠ è½½æˆåŠŸï¼æœåŠ¡å‡†å¤‡å°±ç»ªã€‚")
    AI_ENABLED_BY_FILE = True
except Exception as e:
    print(f"\nâŒ AIæ¨¡å‹åŠ è½½å¤±è´¥: {e}\n   - æœåŠ¡å°†ä»¥æ— AIæ¨¡å¼è¿è¡Œã€‚")

# ========== Flask / Redis / å·¥å…·å‡½æ•° ==========
app = Flask(__name__)
app.config['JSON_AS_ASCII'] = False
CORS(app)
WSGIRequestHandler.protocol_version = "HTTP/1.1"
logging.getLogger('werkzeug').disabled = True
r = redis.Redis(host=REDIS_HOST, port=REDIS_PORT, db=REDIS_DB, decode_responses=True)
http = requests.Session()
job_q = queue.Queue(maxsize=MAX_QUEUE_SIZE)
img_pool = ThreadPoolExecutor(max_workers=IMG_MAX_WORKERS)
shutdown_event = threading.Event()
os.makedirs(IMAGES_ROOT, exist_ok=True)
os.makedirs(WAL_DIR, exist_ok=True)
def now_str(): return datetime.now().strftime("%Y-%m-%d %H:%M:%S")
def parse_dt(s: str) -> Optional[datetime]:
    if not s: return None
    for fmt in ("%Y-m-d %H:%M:%S", "%Y/%m/%d %H:%M:%S", "%Y-m-%d", "%Y/%m/%d"):
        try: return datetime.strptime(s, fmt)
        except: pass
    return None
def parse_count_chinese(s: str) -> int:
    s = str(s); s = s.translate(str.maketrans({**{chr(ord('ï¼')+i): str(i) for i in range(10)}, 'ï¼': '.', 'ï¼Œ': ','})).replace(",", "").rstrip("+").strip()
    if s.endswith("ä¸‡"): return int(round(float(s[:-1]) * 10000)) if s[:-1] else 0
    return int(float(s)) if s and s.replace('.', '', 1).isdigit() else 0
def get_statistical_date_str(dt: datetime = None) -> str:
    dt = dt or datetime.now(); return (dt - timedelta(days=1) if dt.hour < 4 else dt).strftime('%Y-%m-%d')
def update_and_print_daily_stats(status: str):
    try:
        date_str = get_statistical_date_str(); approved_key, rejected_key = f"daily_stats:approved:{date_str}", f"daily_stats:rejected:{date_str}"
        (r.incr(approved_key) if status == "ç¬¦åˆ" else (r.incr(rejected_key) if status == "ä¸ç¬¦åˆ" or status == "äººå·¥å®¡æ ¸" else None))
        r.expire(approved_key, 48 * 3600); r.expire(rejected_key, 48 * 3600)
        approved_count, rejected_count = int(r.get(approved_key) or 0), int(r.get(rejected_key) or 0)
        print(f"ğŸ“Š ä»Šæ—¥ç»Ÿè®¡ (4AM-4AM): ç¬¦åˆ {approved_count} | å…¶ä»– {rejected_count}")
    except Exception as e: print(f"âŒ æ›´æ–°æ¯æ—¥ç»Ÿè®¡å¤±è´¥: {e}")

INFO_SHEET = "åšä¸»ä¿¡æ¯"; NOTES_SHEET = "åšä¸»ç¬”è®°"
APPROVED_COLS = ["ç”¨æˆ·å", "å°çº¢ä¹¦å·", "ä¸»é¡µç½‘å€", "é‚®ç®±", "æœç´¢è¯", "ä¸ªäººç®€ä»‹", "ç²‰ä¸æ•°", "æ€»ç‚¹èµ", "æ ‡è®°æ—¶é—´"]
TRAINING_INFO_COLS = ["ç”¨æˆ·å", "å°çº¢ä¹¦å·", "ä¸»é¡µç½‘å€", "é‚®ç®±", "æœç´¢è¯", "å®¡æ ¸çŠ¶æ€", "AIé¢„æµ‹æ¦‚ç‡", "AIé¢„æµ‹çŠ¶æ€", "ä¸ªäººç®€ä»‹", "ç²‰ä¸æ•°", "æ€»ç‚¹èµ", "æ ‡è®°æ—¶é—´"]
TRAINING_NOTES_COLS = ["å°çº¢ä¹¦å·", "ç¬”è®°åºå·", "ç¬”è®°æ ‡é¢˜", "ç¬”è®°ç‚¹èµæ•°", "ç¬”è®°å°é¢è·¯å¾„", "æ ‡è®°æ—¶é—´"]
REVIEW_COLS = ["URL", "å°çº¢ä¹¦å·", "ç”¨æˆ·å", "æ ‡è®°æ—¶é—´"]
def safe_write_with_lock(xlsx_path: str, writer_func):
    lock_path = xlsx_path + ".lock"
    try:
        with FileLock(lock_path, timeout=1):
            return writer_func(xlsx_path)
    except Timeout: raise IOError(f"è·å–æ–‡ä»¶é”è¶…æ—¶: '{os.path.basename(xlsx_path)}' å¯èƒ½è¢«å ç”¨ã€‚")
    except Exception as e: raise IOError(f"å†™å…¥ '{os.path.basename(xlsx_path)}' æ—¶å‡ºé”™: {e}")

def download_and_convert_image(url: str, userid: str) -> str:
    if not url or not userid: return ""
    try:
        resp = http.get(url, timeout=HTTP_TIMEOUT, stream=True)
        if resp.status_code != 200: return ""
        raw = resp.content; h = hashlib.sha1(raw).hexdigest(); subdir = os.path.join(IMAGES_ROOT, userid, h[:2]); os.makedirs(subdir, exist_ok=True)
        out_path = os.path.join(subdir, f"{h}.webp")
        if os.path.exists(out_path): return os.path.relpath(out_path).replace("\\", "/")
        im = Image.open(io.BytesIO(raw)).convert("RGB"); w, hgt = im.size
        if max(w, hgt) > IMAGE_MAX_SIDE: ratio = IMAGE_MAX_SIDE / max(w, hgt); im = im.resize((int(w * ratio), int(hgt * ratio)), Image.LANCZOS)
        im.save(out_path, format=IMAGE_FORMAT, quality=IMAGE_QUALITY, method=6)
        return os.path.relpath(out_path).replace("\\", "/")
    except Exception: return ""

# ========== åå°ä»»åŠ¡å¤„ç† ==========
class ApprovedBatcher:
    def __init__(self):
        self.lock = threading.Lock(); self.info_rows: List[Dict] = []; self.last_flush_time = time.time()
    def add(self, info_row: Dict):
        with self.lock: self.info_rows.append(info_row)
    def flush(self, force: bool = False):
        with self.lock:
            now = time.time()
            if not force and (not self.info_rows or (len(self.info_rows) < BATCH_FLUSH_ROWS and now - self.last_flush_time < BATCH_FLUSH_SEC)): return
            rows_to_write = list(self.info_rows); self.info_rows = self.info_rows[len(rows_to_write):]
        if not rows_to_write: return
        def writer(path):
            if not os.path.exists(path):
                wb = Workbook(); ws = wb.active; ws.title = INFO_SHEET; ws.append(APPROVED_COLS); wb.save(path)
            wb = load_workbook(path); ws = wb.active
            for row_dict in rows_to_write: ws.append([row_dict.get(h, "") for h in APPROVED_COLS])
            wb.save(path)
        try:
            safe_write_with_lock(APPROVED_EXCEL_PATH, writer)
            print(f"ğŸ“¦ (åå°)æ‰¹é‡å†™å…¥ 'å·²é€šè¿‡' {len(rows_to_write)} æ¡æˆåŠŸã€‚")
        except Exception as e: print(f"âš ï¸ (åå°)æ‰¹é‡å†™å…¥ 'å·²é€šè¿‡' å¤±è´¥ï¼Œå°†é‡è¯•: {e}")
approved_batcher = ApprovedBatcher()

def save_for_approved(job: Dict):
    info_row_map = {h: h for h in ["ç”¨æˆ·å", "å°çº¢ä¹¦å·", "ä¸»é¡µç½‘å€", "é‚®ç®±", "æœç´¢è¯", "ä¸ªäººç®€ä»‹", "ç²‰ä¸æ•°", "æ€»ç‚¹èµ", "æ ‡è®°æ—¶é—´"]}
    info_row_map.update({"ç”¨æˆ·å": "username", "å°çº¢ä¹¦å·": "userid", "ä¸»é¡µç½‘å€": "url", "ä¸ªäººç®€ä»‹": "bio", "ç²‰ä¸æ•°": "followers", "æ€»ç‚¹èµ": "likes_total", "æ ‡è®°æ—¶é—´": "timestamp", "æœç´¢è¯": "search_term"})
    info_data = {k: job.get(v, "") for k, v in info_row_map.items()}
    approved_batcher.add(info_data)

def save_for_review(job: Dict):
    row = [job.get("url", ""), job.get("userid", ""), job.get("username", ""), job.get("timestamp", "")]
    def writer(path):
        if not os.path.exists(path):
            wb = Workbook(); ws = wb.active; ws.append(REVIEW_COLS); wb.save(path)
        wb = load_workbook(path); wb.active.append(row); wb.save(path)
    safe_write_with_lock(MANUAL_REVIEW_EXCEL_PATH, writer)

def save_to_fetched_list(job: Dict):
    username, userid = job.get("username"), job.get("userid")
    def writer(path):
        sheet_name_user, sheet_name_id = "çˆ¬å–ç”¨æˆ·å", "å°çº¢ä¹¦å·"
        if not os.path.exists(path):
            wb = Workbook(); ws_user = wb.active; ws_user.title = sheet_name_user; ws_user.append([sheet_name_user])
            ws_id = wb.create_sheet(sheet_name_id); ws_id.append([sheet_name_id]); wb.save(path)
        wb = load_workbook(path)
        if username: (wb[sheet_name_user] if sheet_name_user in wb.sheetnames else wb.create_sheet(sheet_name_user, 0)).append([username])
        if userid: (wb[sheet_name_id] if sheet_name_id in wb.sheetnames else wb.create_sheet(sheet_name_id, 1)).append([str(userid)])
        wb.save(path)
    safe_write_with_lock(FETCHED_USER_LIST_PATH, writer)

def save_for_training(job: Dict):
    info_row_map = {h: h for h in TRAINING_INFO_COLS}
    info_row_map.update({"ç”¨æˆ·å": "username", "å°çº¢ä¹¦å·": "userid", "ä¸»é¡µç½‘å€": "url", "å®¡æ ¸çŠ¶æ€": "status", "AIé¢„æµ‹æ¦‚ç‡": "ai_prob", "AIé¢„æµ‹çŠ¶æ€": "ai_decision", "ä¸ªäººç®€ä»‹": "bio", "ç²‰ä¸æ•°": "followers", "æ€»ç‚¹èµ": "likes_total", "æ ‡è®°æ—¶é—´": "timestamp", "æœç´¢è¯": "search_term"})
    info_row = {k: job.get(v, "") for k, v in info_row_map.items()}
    notes_rows = []; userid = job.get("userid") or job.get("username") or "unknown"
    futures = {i: img_pool.submit(download_and_convert_image, n.get("cover_url", ""), userid) for i, n in enumerate(job.get("notes", [])[:NUM_NOTES_TO_PROCESS]) if n.get("cover_url")}
    covers = {i: fut.result() for i, fut in futures.items()}
    for i, n in enumerate(job.get("notes", [])[:NUM_NOTES_TO_PROCESS]): notes_rows.append([job.get("userid", ""), i + 1, n.get("title", ""), parse_count_chinese(str(n.get("likes", "0"))), covers.get(i, ""), job.get("timestamp", "")])
    def writer(path):
        if not os.path.exists(path):
            wb = Workbook(); ws_info = wb.active; ws_info.title = INFO_SHEET; ws_info.append(TRAINING_INFO_COLS)
            ws_notes = wb.create_sheet(NOTES_SHEET); ws_notes.append(TRAINING_NOTES_COLS); wb.save(path)
        wb = load_workbook(path)
        ws_info = wb[INFO_SHEET] if INFO_SHEET in wb.sheetnames else wb.create_sheet(INFO_SHEET, 0)
        ws_info.append([info_row.get(h, "") for h in TRAINING_INFO_COLS])
        if notes_rows:
            ws_notes = wb[NOTES_SHEET] if NOTES_SHEET in wb.sheetnames else wb.create_sheet(NOTES_SHEET, 1)
            for r in notes_rows: ws_notes.append(r)
        wb.save(path)
    safe_write_with_lock(NEW_TRAINING_DATA_EXCEL, writer)

def process_job(job: Dict):
    job_id = job.get("job_id")
    if job_id and r.sismember(WAL_DONE_SET, job_id): return
    status = job.get("status")
    if status == "ç¬¦åˆ":
        try: save_for_approved(job)
        except Exception as e: print(f"âŒ [åå°] ä¿å­˜ 'å·²é€šè¿‡' æ•°æ®å¤±è´¥: {e}")
    if status == "äººå·¥å®¡æ ¸":
        try: save_for_review(job); print(f"ğŸ“‹ (åå°)å·²ä¿å­˜è‡³å¾…å¤å®¡: {job.get('username') or job.get('userid')}")
        except Exception as e: print(f"âŒ [åå°] ä¿å­˜ 'å¾…å¤å®¡' æ•°æ®å¤±è´¥: {e}")
    try: save_to_fetched_list(job)
    except Exception as e: print(f"âš ï¸ [åå°] å†™å…¥Excelå»é‡åˆ—è¡¨å¤±è´¥: {e}")
    save_history_enabled = r.get(SAVE_HISTORY_ENABLED_KEY) == "1"
    if status == "ç¬¦åˆ" or (status == "ä¸ç¬¦åˆ" and save_history_enabled):
        try: save_for_training(job)
        except Exception as e: print(f"âš ï¸ [åå°] å†™å…¥è®­ç»ƒæ•°æ®å¤±è´¥: {e}")
    if job_id: r.sadd(WAL_DONE_SET, job_id)

def consume_jobs():
    print("âœ… åå°æ¶ˆè´¹çº¿ç¨‹å·²å¯åŠ¨...")
    while not shutdown_event.is_set():
        try:
            job = job_q.get(timeout=1)
            process_job(job)
            job_q.task_done()
        except queue.Empty:
            approved_batcher.flush()
            continue
        except Exception as e: print(f"âŒ å¤„ç†åå°ä»»åŠ¡æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}"); time.sleep(2)

# ========== AI é¢„æµ‹æ ¸å¿ƒå‡½æ•° ==========
def run_ai_prediction(blogger_data: Dict) -> (float, str, str):
    if not AI_ENABLED_BY_FILE: return 0.0, "error", "AIæ¨¡å‹æœªå¯ç”¨æˆ–åŠ è½½å¤±è´¥"
    try:
        text_dim=TEXT_EMBEDDING_MODEL.get_sentence_embedding_dimension()or 768;image_dim=IMAGE_EMBEDDING_MODEL.get_sentence_embedding_dimension()or 512;bio=str(blogger_data.get('bio',''));notes_data=blogger_data.get('notes',[]);found_email=extract_and_normalize_email(bio);has_email=1 if found_email else 0;followers=parse_count_chinese(str(blogger_data.get('followers',0)));total_likes=parse_count_chinese(str(blogger_data.get('likes_total',0)));avg_likes_per_fan=total_likes/(followers+1)if followers>0 else 0;s_ratio,d_ratio=0,0
        if notes_data:likes_list=[parse_count_chinese(str(n.get('likes','0')))for n in notes_data[:NUM_NOTES_TO_PROCESS]];s_ratio=sum(1 for l in likes_list if l<10)/len(likes_list)if likes_list else 0;d_ratio=sum(1 for l in likes_list if 10<=l<100)/len(likes_list)if likes_list else 0
        bio_vec=TEXT_EMBEDDING_MODEL.encode([bio],convert_to_tensor=True,device=AI_DEVICE);titles=[n.get('title','')for n in notes_data[:NUM_NOTES_TO_PROCESS]];title_vec=TEXT_EMBEDDING_MODEL.encode(titles,convert_to_tensor=True,device=AI_DEVICE).mean(axis=0,keepdim=True)if titles else torch.zeros((1,text_dim),device=AI_DEVICE);image_urls=[note.get('cover_url')for note in notes_data if note.get('cover_url')];image_vec=torch.zeros((1,image_dim),device=AI_DEVICE)
        if image_urls:
            pil_images=[]
            for url in image_urls[:NUM_NOTES_TO_PROCESS]:
                if url:
                    try:pil_images.append(Image.open(io.BytesIO(http.get(url,timeout=5).content)).convert("RGB"))
                    except:pass
            if pil_images:image_vec=IMAGE_EMBEDDING_MODEL.encode(pil_images,convert_to_tensor=True,device=AI_DEVICE).mean(axis=0,keepdim=True)
        numeric_features=np.array([[has_email,followers,total_likes,avg_likes_per_fan,s_ratio,d_ratio]],dtype=np.float32);full_feature_np=np.concatenate([numeric_features,bio_vec.cpu().numpy(),title_vec.cpu().numpy(),image_vec.cpu().numpy()],axis=1)
        if full_feature_np.shape[1]<len(AI_SCALER.feature_names_in_):padded_arr=np.zeros((1,len(AI_SCALER.feature_names_in_)));padded_arr[:,:full_feature_np.shape[1]]=full_feature_np;full_feature_np=padded_arr
        scaled_features=AI_SCALER.transform(pd.DataFrame(full_feature_np,columns=AI_SCALER.feature_names_in_))
        with torch.no_grad():probability=AI_MODEL(torch.tensor(scaled_features,dtype=torch.float32).to(AI_DEVICE)).item()
        if probability>AI_ACCEPT_THRESHOLD:decision="ç¬¦åˆ";reason=f"æ¨¡å‹æ¦‚ç‡ {probability:.2f} > {AI_ACCEPT_THRESHOLD}"
        elif probability>=AI_REJECT_THRESHOLD:decision="äººå·¥å®¡æ ¸";reason=f"æ¨¡å‹æ¦‚ç‡ {probability:.2f} åœ¨ [{AI_REJECT_THRESHOLD}, {AI_ACCEPT_THRESHOLD}] ä¹‹é—´"
        else:decision="ä¸ç¬¦åˆ";reason=f"æ¨¡å‹æ¦‚ç‡ {probability:.2f} < {AI_REJECT_THRESHOLD}"
        return round(probability,4),decision,reason
    except Exception as e:
        print(f"âŒ AI Prediction Error: {e}");return 0.0,"error",str(e)

def load_fetched_list_to_redis():
    if not os.path.exists(FETCHED_USER_LIST_PATH): print(f"â„¹ï¸ æœªæ‰¾åˆ° '{FETCHED_USER_LIST_PATH}'ï¼Œè·³è¿‡åŠ è½½ã€‚"); return
    print(f"æ­£åœ¨ä» '{FETCHED_USER_LIST_PATH}' åŠ è½½å·²æœ‰ç”¨æˆ·åˆ°Redis...");
    try:
        wb=load_workbook(FETCHED_USER_LIST_PATH,read_only=True);pipe=r.pipeline();user_count=0;id_count=0
        if"çˆ¬å–ç”¨æˆ·å"in wb.sheetnames:ws_user=wb["çˆ¬å–ç”¨æˆ·å"];users_to_add=[row[0]for row in ws_user.iter_rows(min_row=2,values_only=True)if row and row[0] and row[0].strip()];pipe.sadd(USERNAMES_SET_KEY,*users_to_add);user_count=len(users_to_add)
        if"å°çº¢ä¹¦å·"in wb.sheetnames:ws_id=wb["å°çº¢ä¹¦å·"];ids_to_add=[str(row[0])for row in ws_id.iter_rows(min_row=2,values_only=True)if row and row[0] and str(row[0]).strip()];pipe.sadd(USERIDS_SET_KEY,*ids_to_add);id_count=len(ids_to_add)
        pipe.execute();print(f"âœ… ä»å·²æœ‰åå•åŠ è½½å®Œæˆ: {user_count}ä¸ªç”¨æˆ·å, {id_count}ä¸ªå°çº¢ä¹¦å·ã€‚")
    except Exception as e:print(f"âŒ ä» '{FETCHED_USER_LIST_PATH}' åŠ è½½å¤±è´¥: {e}")

# ========== API è·¯ç”± (æ ¸å¿ƒä¿®æ”¹å¤„) ==========
@app.route("/usernames", methods=["GET"])
def get_usernames(): return jsonify(list(r.smembers(USERNAMES_SET_KEY)))
@app.route("/userids", methods=["GET"])
def get_userids(): return jsonify(list(r.smembers(USERIDS_SET_KEY)))

# ========== ã€æ ¸å¿ƒä¿®å¤ Iã€‘ ==========
@app.route("/mark_data", methods=["POST"])
def mark_data():
    data = request.get_json(silent=True) or {}
    print(f"[{now_str()}] æ”¶åˆ° /mark_data åŸå§‹è¯·æ±‚: user=({data.get('username')}|{data.get('userid')}), status={data.get('status')}")

    final_decision = (data.get("status") or "").strip()
    if not final_decision: 
        ai_prob, ai_decision, _ = run_ai_prediction(data)
        final_decision = ai_decision
        data['status'] = final_decision
        data['ai_prob'] = ai_prob
        data['ai_decision'] = ai_decision
    else:
        ai_prob, ai_decision, _ = run_ai_prediction(data)
        data['ai_prob'] = ai_prob
        data['ai_decision'] = ai_decision

    username, userid = (data.get("username") or "").strip(), (data.get("userid") or "").strip()
    if not final_decision or (not username and not userid): 
        return jsonify({"status": "error", "message": "ç¼ºå°‘å…³é”®å‚æ•°"}), 400
    
    with mark_data_lock:
        is_uname_member = r.sismember(USERNAMES_SET_KEY, username) if username else False
        is_uid_member = r.sismember(USERIDS_SET_KEY, str(userid)) if userid else False
        
        # ã€é€»è¾‘å˜æ›´ã€‘å¦‚æœç”¨æˆ·å·²åœ¨å»é‡åº“ä¸­ï¼Œå¹¶ä¸”æ–°çŠ¶æ€ä¸æ˜¯â€œäººå·¥å®¡æ ¸â€ï¼Œåˆ™è§†ä¸ºé‡å¤å¹¶æ‹¦æˆªã€‚
        # è¿™ä¸ªä¿®æ”¹å…è®¸ä¸€ä¸ªä¹‹å‰è¢«åˆ¤ä¸ºâ€œä¸ç¬¦åˆâ€çš„ç”¨æˆ·ï¼Œå¯ä»¥è¢«å†æ¬¡æäº¤ä¸ºâ€œäººå·¥å®¡æ ¸â€çŠ¶æ€ã€‚
        if (is_uname_member or is_uid_member) and final_decision != "äººå·¥å®¡æ ¸":
            print(f"âš ï¸ æ‹¦æˆªåˆ°é‡å¤ä¿å­˜è¯·æ±‚(å·²åœ¨Redisä¸­): user=({username}|{userid})")
            return jsonify({"status": "duplicated"})
        
        # ã€é€»è¾‘å˜æ›´ã€‘åªæœ‰å½“æœ€ç»ˆçŠ¶æ€æ˜¯â€œç¬¦åˆâ€æˆ–â€œä¸ç¬¦åˆâ€æ—¶ï¼Œæ‰å°†ç”¨æˆ·åŠ å…¥å»é‡åº“ã€‚
        # â€œäººå·¥å®¡æ ¸â€çŠ¶æ€çš„ç”¨æˆ·ä¸ä¼šè¢«åŠ å…¥ï¼Œä»¥ä¾¿å°†æ¥è¿˜èƒ½è¢«å†æ¬¡å®¡æ ¸ã€‚
        if final_decision in ["ç¬¦åˆ", "ä¸ç¬¦åˆ"]:
            if username: r.sadd(USERNAMES_SET_KEY, username)
            if userid: r.sadd(USERIDS_SET_KEY, str(userid))
            print(f"âœ… (åŒæ­¥)ç”¨æˆ· {username}|{userid} å·²å†™å…¥Rediså»é‡é›†åˆã€‚")

    if not data.get("email"): data['email'] = extract_and_normalize_email(data.get("bio", "")) or ""
    ts = now_str()
    job_id = f"{userid or username}:{int(time.time()*1000)}:{final_decision}"
    job = {**data, "job_id": job_id, "timestamp": ts, "status": final_decision}
    
    try:
        with open(WAL_FILE, "a", encoding="utf-8") as f: f.write(json.dumps(job, ensure_ascii=False) + "\n")
        job_q.put(job, timeout=0.1)
    except queue.Full: 
        print(f"âŒ æœåŠ¡ç¹å¿™ï¼Œåå°é˜Ÿåˆ—å·²æ»¡ï¼Œæ— æ³•å¤„ç†: user=({username}|{userid})")
        return jsonify({"status": "error", "message": "æœåŠ¡ç¹å¿™ï¼Œé˜Ÿåˆ—å·²æ»¡"}), 503
    except Exception as e: 
        print(f"âŒ å†™å…¥WALæˆ–å…¥é˜Ÿæ—¶å‡ºé”™: {e}")
        return jsonify({"status": "error", "message": "å†…éƒ¨é”™è¯¯"}), 500
    
    print(f"    -> ä»»åŠ¡å·²å…¥é˜Ÿ: {final_decision} (AIé¢„æµ‹: {ai_decision}, P={ai_prob:.4f}), user=({username}|{userid})")
    update_and_print_daily_stats(final_decision)
    return jsonify({"status": "ok"})

@app.route("/ai/settings", methods=["POST"])
def ai_settings(): enabled=bool(request.get_json(silent=True).get("enabled",False));r.set(AI_ENABLED_KEY,"1" if enabled else "0");print(f"âš™ï¸ AIè‡ªåŠ¨å®¡æ ¸å·² {'å¼€å¯' if enabled else 'å…³é—­'}");return jsonify({"status":"ok","ai_enabled":enabled})
@app.route("/settings/save_history", methods=["POST"])
def save_history_settings(): enabled=bool(request.get_json(silent=True).get("enabled",False));r.set(SAVE_HISTORY_ENABLED_KEY,"1" if enabled else "0");print(f"âš™ï¸ ä¿å­˜â€œä¸ç¬¦åˆâ€çš„è®­ç»ƒæ•°æ®å·² {'å¼€å¯' if enabled else 'å…³é—­'}");return jsonify({"status":"ok","save_history_enabled":enabled})
@app.route("/ai/decide", methods=["POST"])
def ai_decide(): data=request.get_json(silent=True);p,dec,why=run_ai_prediction(data);data['status']=dec;threading.Thread(target=mark_data_background,args=(data,)).start();return jsonify({"decision":dec,"reason":why,"p_base":p})
def mark_data_background(data): postJSON(f"http://127.0.0.1:{FLASK_PORT}/mark_data", data)
def postJSON(url,obj):requests.post(url,json=obj,timeout=5)
@app.route("/ai/suggest", methods=["POST"])
def ai_predict_only(): blogger_data = request.get_json(silent=True); p, dec, why = run_ai_prediction(blogger_data); return jsonify({"decision": dec, "reason": why, "p_base": p})

# ========== ã€æ ¸å¿ƒä¿®å¤ IIã€‘ ==========
@app.route("/get_review_list", methods=["GET"])
def get_review_list():
    if not os.path.exists(MANUAL_REVIEW_EXCEL_PATH): 
        return jsonify([])
    try:
        def reader(path):
            wb = load_workbook(path)
            ws = wb.active
            if ws.max_row < 2:
                return [] # æ–‡ä»¶å­˜åœ¨ä½†æ²¡æœ‰æ•°æ®
            
            # è¯»å–æ‰€æœ‰URL
            urls = [row[0] for row in ws.iter_rows(min_row=2, max_row=ws.max_row, values_only=True) if row and row[0]]
            
            # ã€ç¨³å¥æ€§ä¿®æ”¹ã€‘åˆ é™¤æ•°æ®è¡Œï¼Œè€Œä¸æ˜¯é‡å»ºæ–‡ä»¶
            if urls:
                ws.delete_rows(2, len(urls))
                wb.save(path)
            
            return urls

        urls = safe_write_with_lock(MANUAL_REVIEW_EXCEL_PATH, reader) or []
        print(f"âœ… æä¾›äº† {len(urls)} ä¸ªå¾…å¤å®¡URLï¼Œå¹¶å·²æ¸…ç©ºåˆ—è¡¨ã€‚")
        return jsonify(urls)
    except Exception as e: 
        print(f"âŒ è¯»å– '{MANUAL_REVIEW_EXCEL_PATH}' å¤±è´¥: {e}")
        return jsonify({"status": "error", "msg": str(e)}), 500

def read_sheet_as_dicts(xlsx_path: str, sheet_name: str) -> List[Dict]:
    if not os.path.exists(xlsx_path): return []
    wb=load_workbook(xlsx_path,read_only=True);ws=wb[sheet_name]
    if ws.max_row<2:return[]
    rows=list(ws.iter_rows(values_only=True));headers=[str(h or "").strip()for h in rows[0]];return[dict(zip(headers,r))for r in rows[1:]]
def _filter_delta_rows(rows: List[Dict], min_dt: Optional[datetime]) -> List[Dict]:
    return [d for d in rows if(dt:=parse_dt(str(d.get("æ ‡è®°æ—¶é—´",""))))and dt>min_dt] if min_dt else rows
@app.route("/export_delta", methods=["GET"])
def export_delta():
    dataset=(request.args.get("dataset")or"approved").lower()
    curA_dt=parse_dt(r.get(CUR_APPROVED))if r.exists(CUR_APPROVED)else None
    wb=Workbook();wb.remove(wb.active)
    if dataset in("approved","both"):
        ws_info=wb.create_sheet("å·²é€šè¿‡-åšä¸»ä¿¡æ¯");ws_info.append(APPROVED_COLS)
        if os.path.exists(APPROVED_EXCEL_PATH):
            all_info=read_sheet_as_dicts(APPROVED_EXCEL_PATH,INFO_SHEET)
            for d in _filter_delta_rows(all_info,curA_dt):ws_info.append([d.get(k,"")for k in APPROVED_COLS])
    ts_name=datetime.now().strftime("%Y%m%d_%H%M%S");out_name=f"{DELTA_PREFIX}{ts_name}.xlsx";wb.save(out_name);nowS=now_str()
    if dataset in("approved","both"):r.set(CUR_APPROVED,nowS)
    print(f"ğŸ“¤ å¢é‡å¯¼å‡ºå®Œæˆ: {out_name}, æ¸¸æ ‡å·²æ›´æ–°è‡³ {nowS}");return send_file(out_name,as_attachment=True,download_name=out_name)
@app.route("/rebuild_sets", methods=["POST"])
def rebuild_sets():
    try:
        print("ğŸ” å¼€å§‹ä»æ‰€æœ‰Excelæ–‡ä»¶é‡å»ºRediså»é‡é›†åˆ..."); r.delete(USERNAMES_SET_KEY); r.delete(USERIDS_SET_KEY)
        for path in [APPROVED_EXCEL_PATH, NEW_TRAINING_DATA_EXCEL, MANUAL_REVIEW_EXCEL_PATH]:
            if not os.path.exists(path): continue
            for row in read_sheet_as_dicts(path, INFO_SHEET if path != MANUAL_REVIEW_EXCEL_PATH else "Sheet"):
                if u := (row.get("ç”¨æˆ·å") or row.get("URL") or "").strip(): r.sadd(USERNAMES_SET_KEY, u)
                if i := (row.get("å°çº¢ä¹¦å·") or "").strip(): r.sadd(USERIDS_SET_KEY, str(i))
        load_fetched_list_to_redis()
        final_user_count,final_id_count=r.scard(USERNAMES_SET_KEY),r.scard(USERIDS_SET_KEY)
        print(f"âœ… Rediså»é‡é›†åˆå·²é‡å»ºå®Œæˆ: {final_user_count} ç”¨æˆ·å, {final_id_count} å°çº¢ä¹¦å·ã€‚")
        return jsonify({"status":"ok","usernames":final_user_count,"userids":final_id_count})
    except Exception as e:return jsonify({"status":"error","msg":str(e)}),500
@app.route('/dashboard', methods=['GET'])
def dashboard_page():
    try:
        with open('dashboard.html','r',encoding='utf-8')as f:return render_template_string(f.read())
    except FileNotFoundError:return"Error: dashboard.html not found.",404
@app.route("/dashboard_stats", methods=["GET"])
def dashboard_stats():
    try:
        today_str=get_statistical_date_str();approved_today=int(r.get(f"daily_stats:approved:{today_str}")or 0);rejected_today=int(r.get(f"daily_stats:rejected:{today_str}")or 0);yesterday_dt=datetime.now()-timedelta(days=1);yesterday_str=get_statistical_date_str(yesterday_dt);approved_yesterday=int(r.get(f"daily_stats:approved:{yesterday_str}")or 0);rejected_yesterday=int(r.get(f"daily_stats:rejected:{yesterday_str}")or 0);pending_review_count=0
        if os.path.exists(MANUAL_REVIEW_EXCEL_PATH):
            def reader(path): return max(0,load_workbook(path).active.max_row-1)
            pending_review_count = safe_write_with_lock(MANUAL_REVIEW_EXCEL_PATH, reader) or 0
        return jsonify({"today":{"approved":approved_today,"rejected":rejected_today,"total":approved_today+rejected_today,},"yesterday":{"total":approved_yesterday+rejected_yesterday,},"pending_review":pending_review_count})
    except Exception as e:return jsonify({"status":"error","msg":str(e)}),500
@app.route('/open_folder', methods=['POST'])
def open_folder_route():
    key=(request.get_json()or{}).get('key');FILE_MAP={"approved":APPROVED_EXCEL_PATH,"review":MANUAL_REVIEW_EXCEL_PATH,"training":NEW_TRAINING_DATA_EXCEL,"output":os.getcwd()};file_path=FILE_MAP.get(key)
    if not file_path:return jsonify({"status":"error","message":"Invalid file key"}),400
    try:
        folder_path=os.path.dirname(os.path.abspath(file_path))if os.path.isfile(file_path)else os.path.abspath(file_path)
        if not os.path.exists(folder_path):os.makedirs(folder_path,exist_ok=True)
        system=platform.system()
        if system=="Windows":os.startfile(folder_path)
        elif system=="Darwin":subprocess.run(["open",folder_path])
        else:subprocess.run(["xdg-open",folder_path])
        print(f"ğŸ“‚ å·²è¯·æ±‚æ‰“å¼€æ–‡ä»¶å¤¹: {folder_path}");return jsonify({"status":"ok","path":folder_path})
    except Exception as e:return jsonify({"status":"error","message":str(e)}),500
@app.route('/download_file', methods=['GET'])
def download_file_route():
    key=request.args.get('key');FILE_MAP={"approved":APPROVED_EXCEL_PATH,"review":MANUAL_REVIEW_EXCEL_PATH,"training":NEW_TRAINING_DATA_EXCEL};file_path=FILE_MAP.get(key)
    if not file_path:return"Invalid file key",400
    if not os.path.exists(file_path):
        if key == 'approved': wb=Workbook();ws=wb.active;ws.title=INFO_SHEET;ws.append(APPROVED_COLS);wb.save(file_path)
        elif key == 'review': wb=Workbook();ws=wb.active;ws.append(REVIEW_COLS);wb.save(file_path)
        elif key == 'training': wb=Workbook();ws_info=wb.active;ws_info.title=INFO_SHEET;ws_info.append(TRAINING_INFO_COLS);ws_notes=wb.create_sheet(NOTES_SHEET);ws_notes.append(TRAINING_NOTES_COLS);wb.save(file_path)
    return send_file(file_path,as_attachment=True)

# ========== å¯åŠ¨ä¸é€€å‡º ==========
def graceful_shutdown(*args, **kwargs):
    print("\nâ¹ï¸ æ­£åœ¨ä¼˜é›…é€€å‡ºï¼Œè¯·ç¨å€™...")
    shutdown_event.set()
    print("   - ç­‰å¾…ä»»åŠ¡é˜Ÿåˆ—æ¸…ç©º..."); job_q.join()
    print("   - æ­£åœ¨å¼ºåˆ¶åˆ·æ–°æ‰€æœ‰æ‰¹å¤„ç†æ•°æ®åˆ°Excel..."); approved_batcher.flush(force=True)
    print("   - åˆ·æ–°å®Œæˆã€‚")
    print("âœ… åå°ä»»åŠ¡å·²å®‰å…¨å¤„ç†å®Œæ¯•ï¼ŒæœåŠ¡é€€å‡ºã€‚")
    os._exit(0)

if __name__ == "__main__":
    consumer_thread = threading.Thread(target=consume_jobs, daemon=True); consumer_thread.start()
    atexit.register(graceful_shutdown)
    signal.signal(signal.SIGINT, graceful_shutdown); signal.signal(signal.SIGTERM, graceful_shutdown)
    load_fetched_list_to_redis()
    print(f"ğŸš€ [v5.5 - å…³é”®ä¿®å¤ç‰ˆ] æœåŠ¡å¯åŠ¨ï¼šhttp://127.0.0.1:{FLASK_PORT}")
    print(f"ğŸ“Š æ•°æ®é¢æ¿è¯·è®¿é—®: http://localhost:{FLASK_PORT}/dashboard")
    from waitress import serve
    serve(app, host="0.0.0.0", port=FLASK_PORT, threads=16)


# === NEW: åŒæ­¥è¡¥å…¨ç”¨æˆ·å/ID åˆ° Redisï¼Œå¹¶å¼‚æ­¥è¡¥å……åˆ° "å·²çˆ¬å–ç”¨æˆ·å.xlsx" ===
@app.route("/touch_user", methods=["POST"])
def touch_user():
    data = request.get_json(silent=True) or {}
    username = (data.get("username") or "").strip()
    userid   = (data.get("userid") or "").strip()
    if not username and not userid:
        return jsonify({"status":"error","message":"missing username/userid"}), 400

    # 1) åŒæ­¥ï¼šå†™å…¥ Redisï¼ˆç«‹å³ç”Ÿæ•ˆï¼Œç”¨äºå‰ç«¯å®æ—¶å»é‡/é«˜äº®ï¼‰
    with mark_data_lock:
        if username:
            r.sadd(USERNAMES_SET_KEY, username)
        if userid:
            r.sadd(USERIDS_SET_KEY, str(userid))
        print(f"âœ… [/touch_user] åŒæ­¥å†™å…¥ Redis: ({username}|{userid})")

    # 2) å¼‚æ­¥ï¼šExcel å†™å…¥ä»èµ°åå°é˜Ÿåˆ—ï¼ˆå¯åœ¨ç©ºé—²æ—¶æ‰¹é‡è½åœ°ï¼‰
    job = {
        "job_id": f"touch:{userid or username}:{int(time.time()*1000)}",
        "username": username,
        "userid": userid,
        "url": data.get("url",""),
        "timestamp": now_str(),
        "status": "touch"
    }
    try:
        with open(WAL_FILE, "a", encoding="utf-8") as f:
            f.write(json.dumps(job, ensure_ascii=False) + "\\n")
        job_q.put(job, timeout=0.1)
    except Exception as e:
        print(f"âš ï¸ [/touch_user] å…¥é˜Ÿå¤±è´¥: {e}")

    return jsonify({"status":"ok"})
