# predict.py (æœ€ç»ˆç‰ˆ v3.0 - å¸¦äººå·¥å®¡æ ¸åŒº)
# -*- coding: utf-8 -*-
import pandas as pd
import torch
import joblib
import os
import re
from tqdm import tqdm
from sentence_transformers import SentenceTransformer
from PIL import Image
import torch.nn as nn
import warnings
import numpy as np

warnings.filterwarnings("ignore", category=FutureWarning)

# ======================================================================================
# 1. é…ç½®åŒºåŸŸ - æ‚¨ç°åœ¨å¯ä»¥æ§åˆ¶ä¸¤ä¸ªé˜ˆå€¼ï¼
# ======================================================================================
# --- è¾“å…¥å’Œè¾“å‡ºæ–‡ä»¶ ---
INPUT_FILE_TO_PREDICT = 'å…¨éƒ¨æµ‹è¯•æ•°æ®.xlsx' 
OUTPUT_FILE = 'ç­›é€‰ç»“æœ(å¸¦äººå·¥å®¡æ ¸).xlsx' # æœ€ç»ˆè¾“å‡ºæ–‡ä»¶

# --- æ¨¡å‹å’ŒScalerè·¯å¾„ ---
MODEL_PATH = 'blogger_classifier_model.pth'
SCALER_PATH = 'scaler.joblib'

# --- å…³é”®ï¼ä¸‰æ®µå¼å†³ç­–é˜ˆå€¼ ---
UPPER_THRESHOLD = 0.6  # é«˜äºæˆ–ç­‰äºæ­¤å€¼ï¼Œç›´æ¥åˆ¤å®šä¸ºâ€œè‡ªåŠ¨ç¬¦åˆâ€
LOWER_THRESHOLD = 0.4  # ä½äºæˆ–ç­‰äºæ­¤å€¼ï¼Œç›´æ¥åˆ¤å®šä¸ºâ€œè‡ªåŠ¨æ‹’ç»â€
# ä»‹äº UPPER å’Œ LOWER ä¹‹é—´çš„ï¼Œå°†è¢«åˆ¤å®šä¸ºâ€œå¾…äººå·¥å®¡æ ¸â€

# --- å…¶ä»–é…ç½® (ä¿æŒä¸å˜) ---
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
os.environ['HF_HOME'] = './huggingface_cache'
BLOGGER_INFO_SHEET_NAME = 'åšä¸»ä¿¡æ¯'
NOTES_INFO_SHEET_NAME = 'åšä¸»ç¬”è®°'
IMAGE_ROOT_FOLDER = 'å°çº¢ä¹¦å›¾ç‰‡'
TEXT_MODEL_NAME = 'moka-ai/m3e-base'
IMAGE_MODEL_NAME = 'sentence-transformers/clip-ViT-B-32'
NUM_NOTES_TO_PROCESS = 20

# ======================================================================================
# 2. æ ¸å¿ƒä»£ç åŒºåŸŸ - åªæœ‰ä¸€å¤„å°ä¿®æ”¹
# ======================================================================================
# (BloggerClassifier, feature_engineering, embed_features å‡½æ•°éƒ½ä¿æŒä¸å˜ï¼Œè¿™é‡Œçœç•¥ä»¥èŠ‚çœç©ºé—´)
# (æ‚¨å¯ä»¥ç›´æ¥åœ¨æ‚¨ç°æœ‰çš„ predict.py ä¸Šä¿®æ”¹ï¼Œæ— éœ€å¤åˆ¶è¿™äº›å‡½æ•°)
class BloggerClassifier(nn.Module):
    def __init__(self, input_features):
        super(BloggerClassifier, self).__init__()
        self.network = nn.Sequential(
            nn.Linear(input_features, 512), nn.ReLU(), nn.Dropout(0.4),
            nn.Linear(512, 256), nn.ReLU(), nn.Dropout(0.4),
            nn.Linear(256, 128), nn.ReLU(), nn.Dropout(0.4),
            nn.Linear(128, 1), nn.Sigmoid()
        )
    def forward(self, x):
        return self.network(x)

def feature_engineering(df_bloggers, df_notes):
    print("  - Step A: è®¡ç®—åŸºç¡€ç‰¹å¾...")
    final_features_list = []
    notes_grouped = df_notes.groupby('å°çº¢ä¹¦å·')
    for _, blogger in tqdm(df_bloggers.iterrows(), total=len(df_bloggers), desc="    - è®¡ç®—ä¸­"):
        blogger_id = str(blogger['å°çº¢ä¹¦å·'])
        features = {
            'å°çº¢ä¹¦å·': blogger_id, 
            'ç”¨æˆ·å': blogger.get('ç”¨æˆ·å', ''), 
            'ä¸ªäººç®€ä»‹_åŸå§‹': blogger.get('ä¸ªäººç®€ä»‹', ''),
            'åŸå§‹å®¡æ ¸çŠ¶æ€': blogger.get('åŸå§‹å®¡æ ¸çŠ¶æ€', 'æœªçŸ¥')
        }
        bio = str(blogger.get('ä¸ªäººç®€ä»‹', ''))
        features['bio_text'] = bio
        email_pattern = r'[\w\.-]+@[\w\.-]+\.\w+'
        emails_found = re.findall(email_pattern, bio)
        features['has_email'] = 1 if emails_found else 0
        followers = blogger.get('ç²‰ä¸æ•°', 0)
        total_likes = blogger.get('æ€»ç‚¹èµ', 0)
        features['followers'] = followers
        features['total_likes'] = total_likes
        features['avg_likes_per_fan'] = total_likes / (followers + 1)
        if blogger_id in notes_grouped.groups:
            blogger_notes = notes_grouped.get_group(blogger_id).head(NUM_NOTES_TO_PROCESS)
            note_count = len(blogger_notes)
            if note_count > 0:
                likes_list = blogger_notes['ç¬”è®°ç‚¹èµæ•°'].tolist()
                single_digit = sum(1 for like in likes_list if like < 10)
                double_digit = sum(1 for like in likes_list if 10 <= like < 100)
                features['single_digit_likes_ratio'] = single_digit / note_count
                features['double_digit_likes_ratio'] = double_digit / note_count
                features['note_titles'] = blogger_notes['ç¬”è®°æ ‡é¢˜'].astype(str).tolist()
                features['note_image_paths'] = blogger_notes['ç¬”è®°å°é¢è·¯å¾„'].astype(str).tolist()
            else:
                features.update({'single_digit_likes_ratio': 0, 'double_digit_likes_ratio': 0, 'note_titles': [], 'note_image_paths': []})
        else:
            features.update({'single_digit_likes_ratio': 0, 'double_digit_likes_ratio': 0, 'note_titles': [], 'note_image_paths': []})
        final_features_list.append(features)
    return pd.DataFrame(final_features_list)

def embed_features(df_features, device):
    print("  - Step B: è½¬æ¢æ–‡æœ¬å’Œå›¾ç‰‡ä¸ºå‘é‡...")
    text_model = SentenceTransformer(TEXT_MODEL_NAME, device=device)
    image_model = SentenceTransformer(IMAGE_MODEL_NAME, device=device)
    bio_texts = df_features['bio_text'].tolist()
    bio_embeddings = text_model.encode(bio_texts, show_progress_bar=True, convert_to_tensor=True, device=device)
    title_avg_embeddings, image_avg_embeddings = [], []
    for _, row in tqdm(df_features.iterrows(), total=len(df_features), desc="    - è½¬æ¢ä¸­"):
        titles = row['note_titles']
        if titles:
            title_embs = text_model.encode(titles, show_progress_bar=False, convert_to_tensor=True, device=device)
            title_avg_embeddings.append(title_embs.mean(axis=0).cpu().numpy())
        else:
            title_avg_embeddings.append([0] * text_model.get_sentence_embedding_dimension())
        image_paths = row['note_image_paths']
        valid_images = []
        if image_paths:
            for img_path in image_paths:
                full_path = os.path.join(IMAGE_ROOT_FOLDER, str(row['å°çº¢ä¹¦å·']), img_path)
                if os.path.exists(full_path):
                    try:
                        valid_images.append(Image.open(full_path).convert("RGB"))
                    except Exception:
                        pass
        if valid_images:
            image_embs = image_model.encode(valid_images, show_progress_bar=False, convert_to_tensor=True, device=device)
            image_avg_embeddings.append(image_embs.mean(axis=0).cpu().numpy())
        else:
            image_avg_embeddings.append([0] * 512)
    bio_df = pd.DataFrame(bio_embeddings.cpu().numpy(), columns=[f'bio_vec_{i}' for i in range(bio_embeddings.shape[1])])
    title_df = pd.DataFrame(title_avg_embeddings, columns=[f'title_vec_{i}' for i in range(len(title_avg_embeddings[0]))])
    image_df = pd.DataFrame(image_avg_embeddings, columns=[f'image_vec_{i}' for i in range(len(image_avg_embeddings[0]))])
    df_features = df_features.drop(columns=['bio_text', 'note_titles', 'note_image_paths'])
    final_df = pd.concat([df_features.reset_index(drop=True), bio_df, title_df, image_df], axis=1)
    return final_df

def predict_new_data():
    print("å¼€å§‹æ‰§è¡Œè‡ªåŠ¨åŒ–ç­›é€‰æµç¨‹...")
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"æ£€æµ‹åˆ°å¯ç”¨è®¾å¤‡: {device.upper()}")

    print("Step 1/4: åŠ è½½å¾…ç­›é€‰æ•°æ®...")
    df_new_bloggers = pd.read_excel(INPUT_FILE_TO_PREDICT, sheet_name=BLOGGER_INFO_SHEET_NAME)
    df_new_notes = pd.read_excel(INPUT_FILE_TO_PREDICT, sheet_name=NOTES_INFO_SHEET_NAME)
    df_new_bloggers['å°çº¢ä¹¦å·'] = df_new_bloggers['å°çº¢ä¹¦å·'].astype(str)
    df_new_notes['å°çº¢ä¹¦å·'] = df_new_notes['å°çº¢ä¹¦å·'].astype(str)

    print("Step 2/4: æ­£åœ¨è¿›è¡Œç‰¹å¾æå– (æ­¤è¿‡ç¨‹è¾ƒé•¿)...")
    df_features = feature_engineering(df_new_bloggers, df_new_notes)
    df_embedded = embed_features(df_features, device)

    print("Step 3/4: åŠ è½½å·²è®­ç»ƒå¥½çš„æ¨¡å‹å’Œæ•°æ®scaler...")
    scaler = joblib.load(SCALER_PATH)
    model_state_dict = torch.load(MODEL_PATH)
    input_dim = model_state_dict['network.0.weight'].shape[1]
    model = BloggerClassifier(input_features=input_dim)
    model.load_state_dict(model_state_dict)
    model.to(device)
    model.eval()

    print("Step 4/4: æ­£åœ¨æ‰§è¡Œé¢„æµ‹...")
    info_cols = ['å°çº¢ä¹¦å·', 'ç”¨æˆ·å', 'ä¸ªäººç®€ä»‹_åŸå§‹', 'åŸå§‹å®¡æ ¸çŠ¶æ€']
    feature_cols = [col for col in df_embedded.columns if col not in info_cols]
    
    X_predict = df_embedded[feature_cols].copy()
    X_predict_scaled = scaler.transform(X_predict)
    X_predict_tensor = torch.tensor(X_predict_scaled, dtype=torch.float32).to(device)
    
    with torch.no_grad():
        probabilities = model(X_predict_tensor).cpu().numpy().flatten()

    results_df = df_embedded[info_cols].copy()
    results_df['ç¬¦åˆæ¦‚ç‡'] = probabilities
    
    # === å”¯ä¸€çš„ã€æ ¸å¿ƒçš„ä¿®æ”¹åœ¨è¿™é‡Œï¼ ===
    # å®šä¹‰ä¸‰æ®µå¼é€»è¾‘çš„æ¡ä»¶å’Œé€‰æ‹©
    conditions = [
        results_df['ç¬¦åˆæ¦‚ç‡'] >= UPPER_THRESHOLD,
        results_df['ç¬¦åˆæ¦‚ç‡'] <= LOWER_THRESHOLD
    ]
    choices = ['è‡ªåŠ¨ç¬¦åˆ', 'è‡ªåŠ¨æ‹’ç»']
    
    # ä½¿ç”¨ np.select æ¥åº”ç”¨ä¸‰æ®µå¼é€»è¾‘
    results_df['ç­›é€‰å»ºè®®'] = np.select(conditions, choices, default='å¾…äººå·¥å®¡æ ¸')
    
    results_df = results_df.sort_values(by='ç¬¦åˆæ¦‚ç‡', ascending=False)
    
    final_cols = ['å°çº¢ä¹¦å·', 'ç”¨æˆ·å', 'åŸå§‹å®¡æ ¸çŠ¶æ€', 'ç¬¦åˆæ¦‚ç‡', 'ç­›é€‰å»ºè®®', 'ä¸ªäººç®€ä»‹_åŸå§‹']
    results_df = results_df[final_cols]
    
    results_df.to_excel(OUTPUT_FILE, index=False)
    
    print("\n" + "="*40)
    print("ğŸ‰ ç­›é€‰å®Œæˆï¼ç»“æœå·²ä¿å­˜ã€‚ ğŸ‰")
    print(f"è¯·åœ¨é¡¹ç›®æ–‡ä»¶å¤¹ä¸­æŸ¥çœ‹ '{OUTPUT_FILE}' æ–‡ä»¶ã€‚")
    print(f"å½“å‰ä½¿ç”¨çš„å†³ç­–é€»è¾‘æ˜¯: >= {UPPER_THRESHOLD} -> è‡ªåŠ¨ç¬¦åˆ, <= {LOWER_THRESHOLD} -> è‡ªåŠ¨æ‹’ç», ä¸­é—´ -> å¾…äººå·¥å®¡æ ¸")
    print("="*40)

if __name__ == "__main__":
    predict_new_data()
