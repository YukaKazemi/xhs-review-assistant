# -*- coding: utf-8 -*-
import pandas as pd
import torch
import joblib
import os
import re
from tqdm import tqdm
from sentence_transformers import SentenceTransformer
from PIL import Image
import torch.nn as nn
import warnings
import numpy as np # <--- è¿™é‡Œæ˜¯å”¯ä¸€çš„ã€å…³é”®çš„è¡¥å……ï¼

warnings.filterwarnings("ignore", category=FutureWarning)

# ======================================================================================
# 1. é…ç½®åŒºåŸŸ - è¿™æ˜¯æ‚¨å”¯ä¸€éœ€è¦ä¿®æ”¹çš„åœ°æ–¹
# ======================================================================================
# --- è¾“å…¥å’Œè¾“å‡ºæ–‡ä»¶ ---
INPUT_FILE_TO_PREDICT = 'å¾…ç­›é€‰åšä¸».xlsx' 
OUTPUT_FILE = 'ç­›é€‰ç»“æœ.xlsx'

# --- æ¨¡å‹å’ŒScalerè·¯å¾„ ---
MODEL_PATH = 'blogger_classifier_model.pth'
SCALER_PATH = 'scaler.joblib'

# --- å…³é”®ï¼å†³ç­–é˜ˆå€¼ ---
DECISION_THRESHOLD = 0.35 

# --- å…¶ä»–é…ç½® (è¯·ä¸ process_data.py ä¿æŒä¸€è‡´) ---
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
os.environ['HF_HOME'] = './huggingface_cache'
BLOGGER_INFO_SHEET_NAME = 'åšä¸»ä¿¡æ¯'
NOTES_INFO_SHEET_NAME = 'åšä¸»ç¬”è®°'
IMAGE_ROOT_FOLDER = 'å°çº¢ä¹¦å›¾ç‰‡'
TEXT_MODEL_NAME = 'moka-ai/m3e-base'
IMAGE_MODEL_NAME = 'sentence-transformers/clip-ViT-B-32'
NUM_NOTES_TO_PROCESS = 20

# ======================================================================================
# 2. æ ¸å¿ƒä»£ç åŒºåŸŸ - æ‚¨é€šå¸¸ä¸éœ€è¦ä¿®æ”¹ä»¥ä¸‹å†…å®¹
# ======================================================================================

# --- ä»è®­ç»ƒè„šæœ¬ä¸­å¤åˆ¶è¿‡æ¥çš„å¿…è¦ç»„ä»¶ ---
# class BloggerClassifier(nn.Module):
# train_model.py (ä¼˜åŒ–ç‰ˆ)
# ... (æ‚¨æ–‡ä»¶é¡¶éƒ¨çš„ import å’Œæ•°æ®åŠ è½½éƒ¨åˆ†ä¿æŒä¸å˜)

# æ‰¾åˆ°æ‚¨å®šä¹‰ BloggerClassifier çš„åœ°æ–¹ï¼Œæ›¿æ¢æˆè¿™ä¸ªæ–°ç‰ˆæœ¬
class BloggerClassifier(nn.Module):
    def __init__(self, input_features):
        super(BloggerClassifier, self).__init__()
        self.network = nn.Sequential(
            nn.Linear(input_features, 512),
            nn.ReLU(),
            # æ ¸å¿ƒä¿®æ”¹ï¼šå°† Dropout æ¯”ä¾‹ä» 0.4 æå‡åˆ° 0.5ï¼Œè¿›è¡Œæ›´å¼ºçš„æ­£åˆ™åŒ–
            nn.Dropout(0.5),
            
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(0.5), # åŒæ ·æå‡
            
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(0.5), # åŒæ ·æå‡

            nn.Linear(128, 1),
            nn.Sigmoid()
        )
    def forward(self, x):
        return self.network(x)

# ... (æ–‡ä»¶å‰©ä½™çš„è®­ç»ƒå¾ªç¯ç­‰éƒ¨åˆ†ä¿æŒä¸å˜)

#     def __init__(self, input_features):
#         super(BloggerClassifier, self).__init__()
#         self.network = nn.Sequential(
#             nn.Linear(input_features, 512), nn.ReLU(), nn.Dropout(0.4),
#             nn.Linear(512, 256), nn.ReLU(), nn.Dropout(0.4),
#             nn.Linear(256, 128), nn.ReLU(), nn.Dropout(0.4),
#             nn.Linear(128, 1), nn.Sigmoid()
#         )
#     def forward(self, x):
#         return self.network(x)

def feature_engineering(df_bloggers, df_notes):
    print("  - Step A: è®¡ç®—åŸºç¡€ç‰¹å¾...")
    final_features_list = []
    notes_grouped = df_notes.groupby('å°çº¢ä¹¦å·')
    for _, blogger in tqdm(df_bloggers.iterrows(), total=len(df_bloggers), desc="    - è®¡ç®—ä¸­"):
        blogger_id = str(blogger['å°çº¢ä¹¦å·'])
        features = {'å°çº¢ä¹¦å·': blogger_id, 'ç”¨æˆ·å': blogger.get('ç”¨æˆ·å', ''), 'ä¸ªäººç®€ä»‹_åŸå§‹': blogger.get('ä¸ªäººç®€ä»‹', '')}
        bio = str(blogger.get('ä¸ªäººç®€ä»‹', ''))
        features['bio_text'] = bio
        email_pattern = r'[\w\.-]+@[\w\.-]+\.\w+'
        emails_found = re.findall(email_pattern, bio)
        features['has_email'] = 1 if emails_found else 0
        followers = blogger.get('ç²‰ä¸æ•°', 0)
        total_likes = blogger.get('æ€»ç‚¹èµ', 0)
        features['followers'] = followers
        features['total_likes'] = total_likes
        features['avg_likes_per_fan'] = total_likes / (followers + 1)
        if blogger_id in notes_grouped.groups:
            blogger_notes = notes_grouped.get_group(blogger_id).head(NUM_NOTES_TO_PROCESS)
            note_count = len(blogger_notes)
            if note_count > 0:
                likes_list = blogger_notes['ç¬”è®°ç‚¹èµæ•°'].tolist()
                single_digit = sum(1 for like in likes_list if like < 10)
                double_digit = sum(1 for like in likes_list if 10 <= like < 100)
                features['single_digit_likes_ratio'] = single_digit / note_count
                features['double_digit_likes_ratio'] = double_digit / note_count
                features['note_titles'] = blogger_notes['ç¬”è®°æ ‡é¢˜'].astype(str).tolist()
                features['note_image_paths'] = blogger_notes['ç¬”è®°å°é¢è·¯å¾„'].astype(str).tolist()
            else:
                features.update({'single_digit_likes_ratio': 0, 'double_digit_likes_ratio': 0, 'note_titles': [], 'note_image_paths': []})
        else:
            features.update({'single_digit_likes_ratio': 0, 'double_digit_likes_ratio': 0, 'note_titles': [], 'note_image_paths': []})
        final_features_list.append(features)
    return pd.DataFrame(final_features_list)

def embed_features(df_features, device):
    print("  - Step B: è½¬æ¢æ–‡æœ¬å’Œå›¾ç‰‡ä¸ºå‘é‡...")
    text_model = SentenceTransformer(TEXT_MODEL_NAME, device=device)
    image_model = SentenceTransformer(IMAGE_MODEL_NAME, device=device)
    bio_texts = df_features['bio_text'].tolist()
    bio_embeddings = text_model.encode(bio_texts, show_progress_bar=True, convert_to_tensor=True, device=device)
    title_avg_embeddings, image_avg_embeddings = [], []
    for _, row in tqdm(df_features.iterrows(), total=len(df_features), desc="    - è½¬æ¢ä¸­"):
        titles = row['note_titles']
        if titles:
            title_embs = text_model.encode(titles, show_progress_bar=False, convert_to_tensor=True, device=device)
            title_avg_embeddings.append(title_embs.mean(axis=0).cpu().numpy())
        else:
            title_avg_embeddings.append([0] * text_model.get_sentence_embedding_dimension())
        image_paths = row['note_image_paths']
        valid_images = []
        if image_paths:
            for img_path in image_paths:
                full_path = os.path.join(IMAGE_ROOT_FOLDER, str(row['å°çº¢ä¹¦å·']), img_path)
                if os.path.exists(full_path):
                    try:
                        valid_images.append(Image.open(full_path).convert("RGB"))
                    except Exception:
                        pass
        if valid_images:
            image_embs = image_model.encode(valid_images, show_progress_bar=False, convert_to_tensor=True, device=device)
            image_avg_embeddings.append(image_embs.mean(axis=0).cpu().numpy())
        else:
            image_avg_embeddings.append([0] * 512)
    bio_df = pd.DataFrame(bio_embeddings.cpu().numpy(), columns=[f'bio_vec_{i}' for i in range(bio_embeddings.shape[1])])
    title_df = pd.DataFrame(title_avg_embeddings, columns=[f'title_vec_{i}' for i in range(len(title_avg_embeddings[0]))])
    image_df = pd.DataFrame(image_avg_embeddings, columns=[f'image_vec_{i}' for i in range(len(image_avg_embeddings[0]))])
    df_features = df_features.drop(columns=['bio_text', 'note_titles', 'note_image_paths'])
    final_df = pd.concat([df_features.reset_index(drop=True), bio_df, title_df, image_df], axis=1)
    return final_df

def predict_new_data():
    print("å¼€å§‹æ‰§è¡Œè‡ªåŠ¨åŒ–ç­›é€‰æµç¨‹...")
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"æ£€æµ‹åˆ°å¯ç”¨è®¾å¤‡: {device.upper()}")

    print("Step 1/4: åŠ è½½å¾…ç­›é€‰æ•°æ®...")
    df_new_bloggers = pd.read_excel(INPUT_FILE_TO_PREDICT, sheet_name=BLOGGER_INFO_SHEET_NAME)
    df_new_notes = pd.read_excel(INPUT_FILE_TO_PREDICT, sheet_name=NOTES_INFO_SHEET_NAME)
    df_new_bloggers['å°çº¢ä¹¦å·'] = df_new_bloggers['å°çº¢ä¹¦å·'].astype(str)
    df_new_notes['å°çº¢ä¹¦å·'] = df_new_notes['å°çº¢ä¹¦å·'].astype(str)

    print("Step 2/4: æ­£åœ¨è¿›è¡Œç‰¹å¾æå– (æ­¤è¿‡ç¨‹è¾ƒé•¿)...")
    df_features = feature_engineering(df_new_bloggers, df_new_notes)
    df_embedded = embed_features(df_features, device)

    print("Step 3/4: åŠ è½½å·²è®­ç»ƒå¥½çš„æ¨¡å‹å’Œæ•°æ®scaler...")
    scaler = joblib.load(SCALER_PATH)
    model_state_dict = torch.load(MODEL_PATH)
    input_dim = model_state_dict['network.0.weight'].shape[1]
    model = BloggerClassifier(input_features=input_dim)
    model.load_state_dict(model_state_dict)
    model.to(device)
    model.eval()

    print("Step 4/4: æ­£åœ¨æ‰§è¡Œé¢„æµ‹...")
    info_cols = ['å°çº¢ä¹¦å·', 'ç”¨æˆ·å', 'ä¸ªäººç®€ä»‹_åŸå§‹']
    feature_cols = [col for col in df_embedded.columns if col not in info_cols]
    X_predict = df_embedded[feature_cols].copy()
    X_predict_scaled = scaler.transform(X_predict)
    X_predict_tensor = torch.tensor(X_predict_scaled, dtype=torch.float32).to(device)
    
    with torch.no_grad():
        probabilities = model(X_predict_tensor).cpu().numpy().flatten()

    results_df = df_embedded[info_cols].copy()
    results_df['ç¬¦åˆæ¦‚ç‡'] = probabilities
    results_df['ç­›é€‰å»ºè®®'] = np.where(results_df['ç¬¦åˆæ¦‚ç‡'] >= DECISION_THRESHOLD, 'å»ºè®®ç¬¦åˆ', 'å»ºè®®æ‹’ç»')

    results_df = results_df.sort_values(by='ç¬¦åˆæ¦‚ç‡', ascending=False)
    
    results_df.to_excel(OUTPUT_FILE, index=False)
    
    print("\n" + "="*40)
    print("ğŸ‰ ç­›é€‰å®Œæˆï¼ç»“æœå·²ä¿å­˜ã€‚ ğŸ‰")
    print(f"è¯·åœ¨é¡¹ç›®æ–‡ä»¶å¤¹ä¸­æŸ¥çœ‹ '{OUTPUT_FILE}' æ–‡ä»¶ã€‚")
    print(f"å½“å‰ä½¿ç”¨çš„å†³ç­–é˜ˆå€¼ä¸º: {DECISION_THRESHOLD} (å¾—åˆ†é«˜äºæ­¤å€¼åˆ™å»ºè®®ç¬¦åˆ)")
    print("æ‚¨å¯ä»¥éšæ—¶ä¿®æ”¹è„šæœ¬é¡¶éƒ¨çš„DECISION_THRESHOLDå€¼æ¥è°ƒæ•´ç­›é€‰çš„å®½æ¾åº¦ã€‚")
    print("="*40)

if __name__ == "__main__":
    predict_new_data()

