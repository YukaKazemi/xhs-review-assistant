# -*- coding: utf-8 -*-
import pandas as pd
import torch
from sentence_transformers import SentenceTransformer, util
from PIL import Image
import os
import re
from tqdm import tqdm

# ======================================================================================
# 1. é…ç½®åŒºåŸŸ - è¯·æ ¹æ®æ‚¨çš„å®é™…æƒ…å†µä¿®æ”¹è¿™é‡Œçš„è·¯å¾„å’Œæ–‡ä»¶å
# ======================================================================================
# è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œä½¿ç”¨å›½å†…é•œåƒæºåŠ é€Ÿæ¨¡å‹ä¸‹è½½
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
os.environ['HF_HOME'] = './huggingface_cache' # å°†æ¨¡å‹ç¼“å­˜åˆ°å½“å‰é¡¹ç›®æ–‡ä»¶å¤¹ä¸‹
# --- æ–‡ä»¶è·¯å¾„é…ç½® ---
POSITIVE_FILE = 'å·²é€šè¿‡æ•°æ®.xlsx'
NEGATIVE_FILE = 'æœªé€šè¿‡æ•°æ®.xlsx'

# --- åšä¸»ä¿¡æ¯å’Œç¬”è®°ä¿¡æ¯åœ¨Excelä¸­çš„å·¥ä½œè¡¨åç§° ---
# æ ¹æ®æ‚¨çš„æ–‡ä»¶ï¼Œåšä¸»ä¿¡æ¯åœ¨'åšä¸»ä¿¡æ¯' sheetï¼Œç¬”è®°åœ¨'åšä¸»ç¬”è®°' sheet
# å¦‚æœæ‚¨çš„sheetåç§°ä¸åŒï¼Œè¯·åœ¨è¿™é‡Œä¿®æ”¹
BLOGGER_INFO_SHEET_NAME = 'åšä¸»ä¿¡æ¯'
NOTES_INFO_SHEET_NAME = 'åšä¸»ç¬”è®°' # å‡è®¾ç¬”è®°æ•°æ®åœ¨åä¸º'åšä¸»ç¬”è®°'çš„sheeté‡Œ

# --- å›¾ç‰‡æ ¹ç›®å½• ---
# è¿™æ˜¯æ‚¨å­˜æ”¾æ‰€æœ‰åšä¸»å›¾ç‰‡çš„å¤§æ–‡ä»¶å¤¹çš„åç§°
IMAGE_ROOT_FOLDER = 'å°çº¢ä¹¦å›¾ç‰‡'

# --- è¾“å‡ºæ–‡ä»¶ ---
# è¿™æ˜¯ç¨‹åºè¿è¡Œåç”Ÿæˆçš„æœ€ç»ˆè®­ç»ƒæ•°æ®æ–‡ä»¶å
OUTPUT_CSV_FILE = 'final_training_data.csv'

# --- æ¨¡å‹é…ç½® ---
# æˆ‘ä»¬ä½¿ç”¨è¿™ä¸ªæ¨¡å‹æ¥å°†æ–‡æœ¬ï¼ˆç®€ä»‹ã€æ ‡é¢˜ï¼‰è½¬æ¢ä¸ºå‘é‡
# é¦–æ¬¡è¿è¡Œæ—¶ä¼šè‡ªåŠ¨ä»ç½‘ä¸Šä¸‹è½½ï¼Œè¯·ä¿æŒç½‘ç»œè¿æ¥
TEXT_MODEL_NAME = 'moka-ai/m3e-base' 
# æˆ‘ä»¬ä½¿ç”¨CLIPæ¨¡å‹æ¥å°†å›¾ç‰‡è½¬æ¢ä¸ºå‘é‡
IMAGE_MODEL_NAME = 'sentence-transformers/clip-ViT-B-32'

# --- å…¶ä»–é…ç½® ---
# æ¯ä¸ªåšä¸»åˆ†æçš„ç¬”è®°æ•°é‡
NUM_NOTES_TO_PROCESS = 20

# ======================================================================================
# 2. æ ¸å¿ƒä»£ç åŒºåŸŸ - æ‚¨é€šå¸¸ä¸éœ€è¦ä¿®æ”¹ä»¥ä¸‹å†…å®¹
# ======================================================================================

def load_and_prepare_data():
    """åŠ è½½'ç¬¦åˆ'å’Œ'ä¸ç¬¦åˆ'çš„æ•°æ®ï¼Œå¹¶æ‰“ä¸Šæ ‡ç­¾"""
    print("Step 1/5: æ­£åœ¨åŠ è½½å¹¶åˆå¹¶åšä¸»æ•°æ®...")
    
    # åŠ è½½åšä¸»ä¿¡æ¯
    df_pos_blogger = pd.read_excel(POSITIVE_FILE, sheet_name=BLOGGER_INFO_SHEET_NAME)
    df_pos_blogger['label'] = 1
    
    df_neg_blogger = pd.read_excel(NEGATIVE_FILE, sheet_name=BLOGGER_INFO_SHEET_NAME)
    df_neg_blogger['label'] = 0
    
    df_bloggers = pd.concat([df_pos_blogger, df_neg_blogger], ignore_index=True)
    print(f"  - æˆåŠŸåˆå¹¶ {len(df_bloggers)} æ¡åšä¸»ä¿¡æ¯ã€‚")

    # åŠ è½½ç¬”è®°ä¿¡æ¯
    print("  - æ­£åœ¨åŠ è½½ç¬”è®°æ•°æ®...")
    df_pos_notes = pd.read_excel(POSITIVE_FILE, sheet_name=NOTES_INFO_SHEET_NAME)
    df_neg_notes = pd.read_excel(NEGATIVE_FILE, sheet_name=NOTES_INFO_SHEET_NAME)
    df_notes = pd.concat([df_pos_notes, df_neg_notes], ignore_index=True)
    # ç¡®ä¿'å°çº¢ä¹¦å·'ç±»å‹ä¸€è‡´ä»¥ä¾¿åˆå¹¶
    df_bloggers['å°çº¢ä¹¦å·'] = df_bloggers['å°çº¢ä¹¦å·'].astype(str)
    df_notes['å°çº¢ä¹¦å·'] = df_notes['å°çº¢ä¹¦å·'].astype(str)
    
    print(f"  - æˆåŠŸåŠ è½½ {len(df_notes)} æ¡ç¬”è®°ä¿¡æ¯ã€‚")
    return df_bloggers, df_notes

def feature_engineering(df_bloggers, df_notes):
    """ä¸ºæ¯ä¸ªåšä¸»è®¡ç®—æ•°å€¼ç‰¹å¾å’Œæ–‡æœ¬ç‰¹å¾"""
    print("Step 2/5: å¼€å§‹è¿›è¡Œç‰¹å¾å·¥ç¨‹...")

    final_features_list = []
    
    # å°†ç¬”è®°æŒ‰åšä¸»åˆ†ç»„ï¼Œæ–¹ä¾¿å¿«é€ŸæŸ¥æ‰¾
    notes_grouped = df_notes.groupby('å°çº¢ä¹¦å·')

    # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦æ¡
    for _, blogger in tqdm(df_bloggers.iterrows(), total=len(df_bloggers), desc="  - å¤„ç†åšä¸»ä¸­"):
        blogger_id = str(blogger['å°çº¢ä¹¦å·'])
        features = {'å°çº¢ä¹¦å·': blogger_id, 'label': blogger['label']}

        # --- ç‰¹å¾1: ä¸ªäººç®€ä»‹å¤„ç† ---
        bio = str(blogger.get('ä¸ªäººç®€ä»‹', ''))
        features['bio_text'] = bio # ä¿å­˜åŸå§‹æ–‡æœ¬ï¼Œåç»­ç»Ÿä¸€ç¼–ç 

        # --- ç‰¹å¾2: é‚®ç®±è¯†åˆ« ---
        # å¼ºå¤§çš„æ­£åˆ™è¡¨è¾¾å¼ï¼Œå¯ä»¥è¯†åˆ«å„ç§è¢«æ··æ·†çš„é‚®ç®±æ ¼å¼
        email_pattern = r'[\w\.-]+@[\w\.-]+\.\w+'
        emails_found = re.findall(email_pattern, bio)
        features['has_email'] = 1 if emails_found else 0

        # --- ç‰¹å¾3: åŸºç¡€æ•°å€¼ç‰¹å¾ ---
        followers = blogger.get('ç²‰ä¸æ•°', 0)
        total_likes = blogger.get('æ€»ç‚¹èµ', 0)
        features['followers'] = followers
        features['total_likes'] = total_likes
        features['avg_likes_per_fan'] = total_likes / (followers + 1) # +1é˜²æ­¢é™¤ä»¥0

        # --- ç‰¹å¾4: ç¬”è®°ç›¸å…³ç‰¹å¾ ---
        if blogger_id in notes_grouped.groups:
            blogger_notes = notes_grouped.get_group(blogger_id).head(NUM_NOTES_TO_PROCESS)
            note_count = len(blogger_notes)

            if note_count > 0:
                likes_list = blogger_notes['ç¬”è®°ç‚¹èµæ•°'].tolist()
                
                # è®¡ç®—ç‚¹èµæ•°åˆ†å¸ƒ
                single_digit = sum(1 for like in likes_list if like < 10)
                double_digit = sum(1 for like in likes_list if 10 <= like < 100)
                
                features['single_digit_likes_ratio'] = single_digit / note_count
                features['double_digit_likes_ratio'] = double_digit / note_count
                
                # ä¿å­˜ç¬”è®°æ ‡é¢˜å’Œå°é¢è·¯å¾„ï¼Œåç»­ç»Ÿä¸€ç¼–ç 
                features['note_titles'] = blogger_notes['ç¬”è®°æ ‡é¢˜'].astype(str).tolist()
                # å‡è®¾å°é¢è·¯å¾„åœ¨'ç¬”è®°å°é¢è·¯'åˆ—ä¸­
                features['note_image_paths'] = blogger_notes['ç¬”è®°å°é¢è·¯å¾„'].astype(str).tolist()
            else:
                # å¦‚æœæ²¡æœ‰ç¬”è®°ï¼Œåˆ™èµ‹äºˆé»˜è®¤å€¼
                features['single_digit_likes_ratio'] = 0
                features['double_digit_likes_ratio'] = 0
                features['note_titles'] = []
                features['note_image_paths'] = []
        else:
            # å¦‚æœåœ¨ç¬”è®°æ•°æ®ä¸­æ‰¾ä¸åˆ°è¯¥åšä¸»ï¼ŒåŒæ ·èµ‹äºˆé»˜è®¤å€¼
            features['single_digit_likes_ratio'] = 0
            features['double_digit_likes_ratio'] = 0
            features['note_titles'] = []
            features['note_image_paths'] = []
        
        final_features_list.append(features)

    return pd.DataFrame(final_features_list)

def embed_features(df_features):
    """ä½¿ç”¨æ¨¡å‹å°†æ–‡æœ¬å’Œå›¾ç‰‡è½¬æ¢ä¸ºå‘é‡"""
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Step 3/5: åŠ è½½æ–‡æœ¬å’Œå›¾åƒæ¨¡å‹åˆ° {device}...")

    text_model = SentenceTransformer(TEXT_MODEL_NAME, device=device)
    image_model = SentenceTransformer(IMAGE_MODEL_NAME, device=device)
    
    print("Step 4/5: å¼€å§‹å°†æ–‡æœ¬å’Œå›¾ç‰‡è½¬æ¢ä¸ºå‘é‡ï¼ˆæ­¤è¿‡ç¨‹å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼‰...")
    
    bio_texts = df_features['bio_text'].tolist()
    bio_embeddings = text_model.encode(bio_texts, show_progress_bar=True, convert_to_tensor=True, device=device)
    
    title_avg_embeddings = []
    image_avg_embeddings = []

    for _, row in tqdm(df_features.iterrows(), total=len(df_features), desc="  - å¤„ç†ç¬”è®°åºåˆ—"):
        titles = row['note_titles']
        if titles:
            title_embs = text_model.encode(titles, show_progress_bar=False, convert_to_tensor=True, device=device)
            title_avg_embeddings.append(title_embs.mean(axis=0).cpu().numpy())
        else:
            title_avg_embeddings.append([0] * text_model.get_sentence_embedding_dimension())

        image_paths = row['note_image_paths']
        valid_images = []
        if image_paths:
            for img_path in image_paths:
                blogger_folder_name = row['å°çº¢ä¹¦å·']
                full_path = os.path.join(IMAGE_ROOT_FOLDER, blogger_folder_name, img_path)
                
                if os.path.exists(full_path):
                    try:
                        valid_images.append(Image.open(full_path).convert("RGB"))
                    except Exception as e:
                        print(f"    - è­¦å‘Š: æ— æ³•æ‰“å¼€å›¾ç‰‡ {full_path}, å·²è·³è¿‡. é”™è¯¯: {e}")
        
        if valid_images:
            image_embs = image_model.encode(valid_images, show_progress_bar=False, convert_to_tensor=True, device=device)
            image_avg_embeddings.append(image_embs.mean(axis=0).cpu().numpy())
        else:
            # ==========================> ç»ˆæä¿®å¤åœ¨è¿™é‡Œ <==========================
            # æˆ‘ä»¬ä¸å†è°ƒç”¨é‚£ä¸ªæœ‰bugçš„å‡½æ•°ï¼Œç›´æ¥ä½¿ç”¨å·²çŸ¥çš„å‘é‡é•¿åº¦ 512
            image_avg_embeddings.append([0] * 512)
            # ====================================================================

    bio_df = pd.DataFrame(bio_embeddings.cpu().numpy(), columns=[f'bio_vec_{i}' for i in range(bio_embeddings.shape[1])])
    title_df = pd.DataFrame(title_avg_embeddings, columns=[f'title_vec_{i}' for i in range(len(title_avg_embeddings[0]))])
    image_df = pd.DataFrame(image_avg_embeddings, columns=[f'image_vec_{i}' for i in range(len(image_avg_embeddings[0]))])
    
    df_features = df_features.drop(columns=['bio_text', 'note_titles', 'note_image_paths'])
    
    final_df = pd.concat([df_features.reset_index(drop=True), bio_df, title_df, image_df], axis=1)
    
    return final_df


def main():
    """ä¸»å‡½æ•°ï¼Œæ‰§è¡Œæ•´ä¸ªæµç¨‹"""
    # 1. åŠ è½½æ•°æ®
    df_bloggers, df_notes = load_and_prepare_data()

    # 2. è®¡ç®—åŸºç¡€ç‰¹å¾
    df_features = feature_engineering(df_bloggers, df_notes)

    # 3. ä½¿ç”¨AIæ¨¡å‹è¿›è¡Œå‘é‡åŒ–
    final_data = embed_features(df_features)

    # 4. ä¿å­˜ç»“æœ
    print(f"Step 5/5: æ‰€æœ‰å¤„ç†å®Œæˆï¼Œæ­£åœ¨ä¿å­˜ç»“æœåˆ° {OUTPUT_CSV_FILE}...")
    final_data.to_csv(OUTPUT_CSV_FILE, index=False, encoding='utf-8-sig')
    print("="*30)
    print("ğŸ‰ æ­å–œï¼æ•°æ®é¢„å¤„ç†æˆåŠŸï¼ ğŸ‰")
    print(f"æœ€ç»ˆçš„è®­ç»ƒæ–‡ä»¶ '{OUTPUT_CSV_FILE}' å·²ç”Ÿæˆåœ¨æ‚¨çš„é¡¹ç›®æ–‡ä»¶å¤¹ä¸­ã€‚")
    print("ä¸‹ä¸€æ­¥ï¼Œæ‚¨å¯ä»¥å°†è¿™ä¸ªæ–‡ä»¶æä¾›ç»™æˆ‘ï¼Œæ¥è®­ç»ƒæœ€ç»ˆçš„åˆ†ç±»æ¨¡å‹ã€‚")
    print("="*30)


if __name__ == "__main__":
    main()

