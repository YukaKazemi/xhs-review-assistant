# xhs_user_spider_ai.py (v5.6 - åŸºäºæ‚¨çš„ v5.5 ä¿®æ”¹)
# - ã€æ ¸å¿ƒä¿®å¤ I - æ•ˆç‡ä¸ç¨³å®šã€‘: å½»åº•é‡æ„ save_to_fetched_list å’Œåå°Excelå†™å…¥é€»è¾‘ã€‚
#   1. Redisæ›´æ–°å˜ä¸ºã€å³æ—¶åŒæ­¥ã€‘ï¼Œç¡®ä¿å†…å­˜æ•°æ®100%å‡†ç¡®ã€‚
#   2. Excelå†™å…¥ä»»åŠ¡äº¤ç”±ã€åå°çº¿ç¨‹æ± ã€‘ä½¿ç”¨ã€Pandasã€‘å¤„ç†ï¼Œæ•ˆç‡å’Œç¨³å®šæ€§è¿œè¶…æ—§ç‰ˆ openpyxl é€è¡ŒåŠ è½½æ¨¡å¼ï¼Œå¤§å¹…é™ä½å¹¶å‘å†²çªã€‚
#   3. æ–°å¢ _perform_save_to_excel å†…éƒ¨å‡½æ•°ï¼Œä¸“èŒè´Ÿè´£å¸¦é”çš„ã€é«˜æ•ˆçš„Excelè¿½åŠ æ“ä½œã€‚
# - ã€æ ¸å¿ƒä¿®å¤ II - å…³é”®æ—¥å¿—ã€‘: åœ¨ save_to_fetched_list ä¸­ï¼Œä¸ºâ€œIDå·²å­˜åœ¨ä½†ç”¨æˆ·åæ˜¯æ–°çš„â€è¿™ä¸€å…³é”®åœºæ™¯æ·»åŠ äº†æ˜ç¡®çš„æ—¥å¿—è¾“å‡ºï¼Œä¾¿äºè¿½è¸ªå†…å­˜æ›´æ–°ã€‚
# - ã€æ ¸å¿ƒä¿®å¤ III - å¥å£®æ€§ã€‘: é‡æ„ load_fetched_list_to_redis å‡½æ•°ï¼Œæ”¹ç”¨Pandaså¹¶å¢åŠ åˆ—åå…¼å®¹å’Œé”™è¯¯å¤„ç†ï¼Œç¡®ä¿æœåŠ¡å¯åŠ¨æ—¶æ•°æ®åŠ è½½çš„å¯é æ€§ã€‚
# - ã€æ¶æ„ä¿ç•™ã€‘: æ‚¨åŸæœ‰çš„AIæ¨¡å‹ã€Redisã€ä»»åŠ¡é˜Ÿåˆ—ã€WALã€æ‰€æœ‰APIè·¯ç”±ç­‰é«˜çº§åŠŸèƒ½å…¨éƒ¨ä¿ç•™ï¼Œæœ¬æ¬¡ä¸ºå¤–ç§‘æ‰‹æœ¯å¼ä¿®å¤ã€‚

# -*- coding: utf-8 -*-
import os
import io
import json
import queue
import atexit
import signal
import hashlib
import logging
import threading
import re
import platform
import subprocess
import time
from datetime import datetime, timedelta
from typing import Optional, List, Dict

from concurrent.futures import ThreadPoolExecutor

import requests
import pandas as pd
from PIL import Image
import redis
from flask import Flask, request, jsonify, Response, send_file, render_template_string
from flask_cors import CORS
from werkzeug.serving import WSGIRequestHandler
from filelock import FileLock, Timeout
import portalocker # ä½¿ç”¨æ›´é€šç”¨çš„æ–‡ä»¶é”

import torch
import joblib
import numpy as np
import torch.nn as nn
from sentence_transformers import SentenceTransformer
from openpyxl import Workbook, load_workbook

# ========== åŸºç¡€é…ç½® (æ¥è‡ªæ‚¨çš„ v5.5) ==========
FLASK_PORT = 5001
APPROVED_EXCEL_PATH = "å·²é€šè¿‡æ•°æ®.xlsx"
MANUAL_REVIEW_EXCEL_PATH = "å¾…å¤å®¡æ•°æ®.xlsx"
NEW_TRAINING_DATA_EXCEL = "å¾…è®­ç»ƒæ•°æ®.xlsx"
DELTA_PREFIX = "å·¥ä½œæˆæœ_"
# ã€ä¿®æ”¹ã€‘å°†æ—§æ–‡ä»¶åä½œä¸ºå¸¸é‡ï¼Œæ–¹ä¾¿å…¼å®¹
FETCHED_USER_LIST_PATH = "å·²çˆ¬å–ç”¨æˆ·å.xlsx" 
# ã€æ–°ã€‘ä¸ºåˆ†ç¦»åçš„æ–‡ä»¶å®šä¹‰æ–°åç§°
FETCHED_USERNAMES_FILE = 'å·²çˆ¬å–ç”¨æˆ·å_v2.xlsx'
FETCHED_USERIDS_FILE = 'å·²çˆ¬å–ç”¨æˆ·ID_v2.xlsx'

IMAGES_ROOT = os.path.join("data", "images")
IMAGE_MAX_SIDE = 768; IMAGE_FORMAT = "WEBP"; IMAGE_QUALITY = 90
REDIS_HOST = "localhost"; REDIS_PORT = 6379; REDIS_DB = 0
BATCH_FLUSH_ROWS = 20; BATCH_FLUSH_SEC  = 10.0; MAX_QUEUE_SIZE = 2000
IMG_MAX_WORKERS  = 6; HTTP_TIMEOUT = (3, 8)
WAL_DIR   = os.path.join("data", "wal_final"); WAL_FILE  = os.path.join(WAL_DIR, "mark_data.jsonl")

# ========== Redis Keys (æ¥è‡ªæ‚¨çš„ v5.5) ==========
AI_ENABLED_KEY = "ai:enabled"
SAVE_HISTORY_ENABLED_KEY = "save_history:enabled"
WAL_DONE_SET = "wal:done_final"
USERNAMES_SET_KEY = "usernames_set"
USERIDS_SET_KEY = "userids_set"
CUR_APPROVED = "export_cursor:approved"
CUR_REJECTED = "export_cursor:rejected"

# ========== é” & çº¿ç¨‹å®‰å…¨ (æ¥è‡ªæ‚¨çš„ v5.5) ==========
mark_data_lock = threading.Lock() 

# ========== é‚®ç®±æå–åŠŸèƒ½ (æ¥è‡ªæ‚¨çš„ v5.5) ==========
EMAIL_MAPPING_DICT = { "è‰¾ç‰¹": "@", " at ": "@", " at": "@", "at ": "@", "Â©": "@", "Â®": "@", "ğŸ¥": "@", "ï¼ ": "@", "(at)": "@", "[at]": "@", "At": "@", "A T": "@", "ğŸ§": "qq", "qqå·": "qq", "æ‰£æ‰£": "qq", "æ‰£": "q", "q ": "q", "ä¼é¹…": "qq", "çƒçƒ": "qq", "163": "163", "ä¸€å…­ä¸‰": "163", "ä¸€äºŒå…­": "126", "126": "126", "æ–°æµª": "sina", "è°·æ­Œ": "gmail", "outlook": "outlook", "hotmail": "hotmail", "ç‚¹": ".", "dian": ".", " dot ": ".", " dot": ".", "dot ": ".", "ã€‚": ".", "Â·": ".", "ä¸¶": ".", " . ": ".", "com": "com", "åº·å§†": "com", "ç‚¹com": ".com", "cn": "cn", "ç‚¹cn": ".cn", "é›¶": "0", "ä¸€": "1", "äºŒ": "2", "ä¸‰": "3", "å››": "4", "äº”": "5", "å…­": "6", "ä¸ƒ": "7", "å…«": "8", "ä¹": "9", }
def extract_and_normalize_email(text: str) -> Optional[str]:
    if not text: return None
    normalized_text = text.lower()
    for non_standard, standard in EMAIL_MAPPING_DICT.items(): normalized_text = normalized_text.replace(non_standard, standard)
    normalized_text = re.sub(r'[\s:ï¼š,ï¼Œ\(\)\[\]]', '', normalized_text)
    match = re.search(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}', normalized_text)
    if match: return match.group(0)
    qq_match = re.search(r'([1-9][0-9]{5,10})qq\.com', normalized_text)
    if qq_match: return f"{qq_match.group(1)}@qq.com"
    return None

# ========== AI æ¨¡å‹åŠ è½½ (æ¥è‡ªæ‚¨çš„ v5.5) ==========
MODEL_PATH = 'blogger_classifier_model.pth'
SCALER_PATH = 'scaler.joblib'
TEXT_MODEL_NAME = r'F:\AI_Model_Project\huggingface_cache\moka-ai_m3e-base'
IMAGE_MODEL_NAME = 'sentence-transformers/clip-ViT-B-32'
NUM_NOTES_TO_PROCESS = 20
AI_REJECT_THRESHOLD = 0.3
AI_ACCEPT_THRESHOLD = 0.55
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
os.environ['HF_HOME'] = './huggingface_cache'
print("ğŸ¤– æ­£åœ¨åˆå§‹åŒ–æœ¬åœ°AIç¯å¢ƒ...")
AI_ENABLED_BY_FILE = False
try:
    if not os.path.exists(MODEL_PATH) or not os.path.exists(SCALER_PATH): raise FileNotFoundError(f"æ¨¡å‹({MODEL_PATH})æˆ–æ ‡å‡†åŒ–æ–‡ä»¶({SCALER_PATH})ä¸å­˜åœ¨ã€‚")
    AI_DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"   - ä½¿ç”¨è®¾å¤‡: {AI_DEVICE.upper()}")
    print(f"   - æ­£åœ¨åŠ è½½åˆ†ç±»å™¨æ¨¡å‹: {MODEL_PATH}")
    model_state_dict = torch.load(MODEL_PATH, map_location=AI_DEVICE, weights_only=True)
    input_dim = model_state_dict['network.0.weight'].shape[1]
    class BloggerClassifier(nn.Module):
        def __init__(self, input_features):
            super(BloggerClassifier, self).__init__(); self.network = nn.Sequential(nn.Linear(input_features, 512), nn.BatchNorm1d(512), nn.ReLU(), nn.Dropout(0.5), nn.Linear(512, 256), nn.BatchNorm1d(256), nn.ReLU(), nn.Dropout(0.5), nn.Linear(256, 128), nn.BatchNorm1d(128), nn.ReLU(), nn.Dropout(0.4), nn.Linear(128, 1), nn.Sigmoid())
        def forward(self, x): return self.network(x)
    AI_MODEL = BloggerClassifier(input_features=input_dim); AI_MODEL.load_state_dict(model_state_dict); AI_MODEL.to(AI_DEVICE); AI_MODEL.eval()
    print("   - åˆ†ç±»å™¨åŠ è½½æˆåŠŸ!")
    print(f"   - æ­£åœ¨åŠ è½½Scaler: {SCALER_PATH}"); AI_SCALER = joblib.load(SCALER_PATH); print("   - ScaleråŠ è½½æˆåŠŸ!")
    print(f"   - æ­£åœ¨åŠ è½½æ–‡æœ¬æ¨¡å‹: {TEXT_MODEL_NAME} ..."); TEXT_EMBEDDING_MODEL = SentenceTransformer(TEXT_MODEL_NAME, device=AI_DEVICE); print("   - æ–‡æœ¬æ¨¡å‹åŠ è½½æˆåŠŸ!")
    print(f"   - æ­£åœ¨åŠ è½½å›¾åƒæ¨¡å‹: {IMAGE_MODEL_NAME} ..."); IMAGE_EMBEDDING_MODEL = SentenceTransformer(IMAGE_MODEL_NAME, device=AI_DEVICE); print("   - å›¾åƒæ¨¡å‹åŠ è½½æˆåŠŸ!")
    print("\nâœ… AIæ¨¡å‹å…¨éƒ¨åŠ è½½æˆåŠŸï¼æœåŠ¡å‡†å¤‡å°±ç»ªã€‚")
    AI_ENABLED_BY_FILE = True
except Exception as e:
    print(f"\nâŒ AIæ¨¡å‹åŠ è½½å¤±è´¥: {e}\n   - æœåŠ¡å°†ä»¥æ— AIæ¨¡å¼è¿è¡Œã€‚")

# ========== Flask / Redis / å·¥å…·å‡½æ•° (æ¥è‡ªæ‚¨çš„ v5.5) ==========
app = Flask(__name__)
app.config['JSON_AS_ASCII'] = False
CORS(app)
WSGIRequestHandler.protocol_version = "HTTP/1.1"
logging.getLogger('werkzeug').disabled = True
# ã€ä¿®æ”¹ã€‘ä½¿ç”¨æ›´è§„èŒƒçš„æ—¥å¿—æ ¼å¼
logging.basicConfig(level=logging.INFO, format='[%(asctime)s] [%(levelname)s] %(message)s', datefmt='%H:%M:%S')

r = redis.Redis(host=REDIS_HOST, port=REDIS_PORT, db=REDIS_DB, decode_responses=True)
http = requests.Session()
job_q = queue.Queue(maxsize=MAX_QUEUE_SIZE)
img_pool = ThreadPoolExecutor(max_workers=IMG_MAX_WORKERS)
# ã€æ–°ã€‘ä¸ºExcelå†™å…¥æ“ä½œåˆ›å»ºä¸€ä¸ªä¸“ç”¨çš„åå°çº¿ç¨‹æ± 
excel_writer_pool = ThreadPoolExecutor(max_workers=1, thread_name_prefix='ExcelWriter')

shutdown_event = threading.Event()
os.makedirs(IMAGES_ROOT, exist_ok=True)
os.makedirs(WAL_DIR, exist_ok=True)

# ... (ä¿ç•™æ‚¨v5.5çš„å·¥å…·å‡½æ•°)
def now_str(): return datetime.now().strftime("%Y-%m-%d %H:%M:%S")
def parse_dt(s: str) -> Optional[datetime]:
    if not s: return None
    for fmt in ("%Y-m-d %H:%M:%S", "%Y/%m/%d %H:%M:%S", "%Y-m-%d", "%Y/%m/%d"):
        try: return datetime.strptime(s, fmt)
        except: pass
    return None
def parse_count_chinese(s: str) -> int:
    s = str(s); s = s.translate(str.maketrans({**{chr(ord('ï¼')+i): str(i) for i in range(10)}, 'ï¼': '.', 'ï¼Œ': ','})).replace(",", "").rstrip("+").strip()
    if s.endswith("ä¸‡"): return int(round(float(s[:-1]) * 10000)) if s[:-1] else 0
    return int(float(s)) if s and s.replace('.', '', 1).isdigit() else 0
def get_statistical_date_str(dt: datetime = None) -> str:
    dt = dt or datetime.now(); return (dt - timedelta(days=1) if dt.hour < 4 else dt).strftime('%Y-%m-%d')
def update_and_print_daily_stats(status: str):
    try:
        date_str = get_statistical_date_str(); approved_key, rejected_key = f"daily_stats:approved:{date_str}", f"daily_stats:rejected:{date_str}"
        (r.incr(approved_key) if status == "ç¬¦åˆ" else (r.incr(rejected_key) if status == "ä¸ç¬¦åˆ" or status == "äººå·¥å®¡æ ¸" else None))
        r.expire(approved_key, 48 * 3600); r.expire(rejected_key, 48 * 3600)
        approved_count, rejected_count = int(r.get(approved_key) or 0), int(r.get(rejected_key) or 0)
        logging.info(f"ğŸ“Š ä»Šæ—¥ç»Ÿè®¡ (4AM-4AM): ç¬¦åˆ {approved_count} | å…¶ä»– {rejected_count}")
    except Exception as e: logging.error(f"âŒ æ›´æ–°æ¯æ—¥ç»Ÿè®¡å¤±è´¥: {e}")

INFO_SHEET = "åšä¸»ä¿¡æ¯"; NOTES_SHEET = "åšä¸»ç¬”è®°"
APPROVED_COLS = ["ç”¨æˆ·å", "å°çº¢ä¹¦å·", "ä¸»é¡µç½‘å€", "é‚®ç®±", "æœç´¢è¯", "ä¸ªäººç®€ä»‹", "ç²‰ä¸æ•°", "æ€»ç‚¹èµ", "æ ‡è®°æ—¶é—´"]
TRAINING_INFO_COLS = ["ç”¨æˆ·å", "å°çº¢ä¹¦å·", "ä¸»é¡µç½‘å€", "é‚®ç®±", "æœç´¢è¯", "å®¡æ ¸çŠ¶æ€", "AIé¢„æµ‹æ¦‚ç‡", "AIé¢„æµ‹çŠ¶æ€", "ä¸ªäººç®€ä»‹", "ç²‰ä¸æ•°", "æ€»ç‚¹èµ", "æ ‡è®°æ—¶é—´"]
TRAINING_NOTES_COLS = ["å°çº¢ä¹¦å·", "ç¬”è®°åºå·", "ç¬”è®°æ ‡é¢˜", "ç¬”è®°ç‚¹èµæ•°", "ç¬”è®°å°é¢è·¯å¾„", "æ ‡è®°æ—¶é—´"]
REVIEW_COLS = ["URL", "å°çº¢ä¹¦å·", "ç”¨æˆ·å", "æ ‡è®°æ—¶é—´"]

# ã€ä¿®æ”¹ã€‘ä¿ç•™æ‚¨çš„ safe_write_with_lock æ¡†æ¶ï¼Œä½†å†…éƒ¨è°ƒç”¨ä¼šæ”¹å˜
def safe_write_with_lock(xlsx_path: str, writer_func):
    lock_path = xlsx_path + ".lock"
    filename = os.path.basename(xlsx_path)
    try:
        with FileLock(lock_path, timeout=5):
            return writer_func(xlsx_path)
    except Timeout:
        logging.error(f"âŒâŒâŒ ä¸¥é‡é”™è¯¯: è·å–æ–‡ä»¶é”è¶…æ—¶ï¼'{filename}' å¯èƒ½æ­£è¢«å…¶ä»–ç¨‹åº(å¦‚WPS/Office)å ç”¨ã€‚")
        raise IOError(f"è·å–æ–‡ä»¶é”è¶…æ—¶: '{filename}' å¯èƒ½è¢«å ç”¨ã€‚")
    except Exception as e:
        logging.error(f"âŒâŒâŒ ä¸¥é‡é”™è¯¯: å†™å…¥ '{filename}' æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        raise IOError(f"å†™å…¥ '{filename}' æ—¶å‡ºé”™: {e}")

# ... (ä¿ç•™æ‚¨v5.5çš„ download_and_convert_image å‡½æ•°)
def download_and_convert_image(url: str, userid: str) -> str:
    if not url or not userid: return ""
    try:
        resp = http.get(url, timeout=HTTP_TIMEOUT, stream=True)
        if resp.status_code != 200: return ""
        raw = resp.content; h = hashlib.sha1(raw).hexdigest(); subdir = os.path.join(IMAGES_ROOT, userid, h[:2]); os.makedirs(subdir, exist_ok=True)
        out_path = os.path.join(subdir, f"{h}.webp")
        if os.path.exists(out_path): return os.path.relpath(out_path).replace("\\", "/")
        im = Image.open(io.BytesIO(raw)).convert("RGB"); w, hgt = im.size
        if max(w, hgt) > IMAGE_MAX_SIDE: ratio = IMAGE_MAX_SIDE / max(w, hgt); im = im.resize((int(w * ratio), int(hgt * ratio)), Image.LANCZOS)
        im.save(out_path, format=IMAGE_FORMAT, quality=IMAGE_QUALITY, method=6)
        return os.path.relpath(out_path).replace("\\", "/")
    except Exception: return ""

# ========== åå°ä»»åŠ¡å¤„ç† (æ ¸å¿ƒä¿®æ”¹åŒº) ==========

# ã€æ–°ã€‘é«˜æ•ˆçš„åå°Excelå†™å…¥å‡½æ•°ï¼Œä½¿ç”¨Pandas
def _perform_save_to_excel(filepath: str, data_list: List[str], column_name: str):
    lock_path = filepath + ".lock"
    try:
        with portalocker.Lock(lock_path, 'a+', timeout=10):
            existing_df = pd.DataFrame()
            if os.path.exists(filepath) and os.path.getsize(filepath) > 0:
                try:
                    existing_df = pd.read_excel(filepath)
                except Exception as e:
                    logging.warning(f"è¯»å–ç°æœ‰ {filepath} å¤±è´¥ï¼Œå°†åˆ›å»ºæ–°æ–‡ä»¶ã€‚é”™è¯¯: {e}")

            new_data_df = pd.DataFrame(data_list, columns=[column_name])
            combined_df = pd.concat([existing_df, new_data_df], ignore_index=True).drop_duplicates(subset=[column_name], keep='last')
            combined_df.to_excel(filepath, index=False)
            logging.info(f"ğŸ’¾ (åå°)æˆåŠŸå°† {len(data_list)} æ¡è®°å½•å†™å…¥ {os.path.basename(filepath)}")
    except portalocker.LockException:
        logging.error(f"âŒâŒâŒ Excelå†™å…¥å¤±è´¥: è·å–æ–‡ä»¶é”è¶…æ—¶! '{filepath}' å¯èƒ½è¢«å ç”¨ã€‚")
    except Exception as e:
        logging.error(f"âŒâŒâŒ åœ¨åå°å†™å…¥ {filepath} æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")

# ã€é‡æ„ã€‘æ–°çš„ save_to_fetched_list å‡½æ•°
def save_to_fetched_list(username: Optional[str] = None, userid: Optional[str] = None):
    """
    v5.6æ ¸å¿ƒå‡½æ•°ï¼šè´Ÿè´£å°†ç”¨æˆ·/IDåŒæ­¥åˆ°Rediså’ŒExcelã€‚
    1. ç«‹å³æ›´æ–°Redis (å†…å­˜)ã€‚
    2. å°†æŒä¹…åŒ–åˆ°Excelçš„ä»»åŠ¡å¼‚æ­¥æäº¤åˆ°åå°ã€‚
    """
    username = username.strip() if username else None
    userid = str(userid).strip() if userid else None

    # 1. å³æ—¶æ›´æ–°Redis
    if username:
        is_uname_new = r.sadd(USERNAMES_SET_KEY, username) == 1
        if is_uname_new:
            excel_writer_pool.submit(_perform_save_to_excel, FETCHED_USERNAMES_FILE, [username], "ç”¨æˆ·å")
    if userid:
        is_uid_new = r.sadd(USERIDS_SET_KEY, userid) == 1
        if is_uid_new:
            excel_writer_pool.submit(_perform_save_to_excel, FETCHED_USERIDS_FILE, [userid], "å°çº¢ä¹¦å·")
    
    # 2. ã€å…³é”®æ—¥å¿—ã€‘å›åº”æ‚¨çš„æ ¸å¿ƒé—®é¢˜
    if username and userid:
        was_id_present = not is_uid_new
        if was_id_present and is_uname_new:
            logging.info(f"âœ¨ å…³é”®åœºæ™¯: ID '{userid}' å·²å­˜åœ¨ï¼Œæ–°å¢å…³è”ç”¨æˆ·å '{username}' åˆ°Rediså’ŒExcelã€‚")


class ApprovedBatcher: # ä¿ç•™æ‚¨çš„å®ç°
    def __init__(self):
        self.lock = threading.Lock(); self.info_rows: List[Dict] = []; self.last_flush_time = time.time()
    def add(self, info_row: Dict):
        with self.lock: self.info_rows.append(info_row)
    def flush(self, force: bool = False):
        with self.lock:
            now = time.time()
            if not force and (not self.info_rows or (len(self.info_rows) < BATCH_FLUSH_ROWS and now - self.last_flush_time < BATCH_FLUSH_SEC)): return
            rows_to_write = list(self.info_rows); self.info_rows = self.info_rows[len(rows_to_write):]
        if not rows_to_write: return
        
        def writer(path):
            if not os.path.exists(path):
                wb = Workbook(); ws = wb.active; ws.title = INFO_SHEET; ws.append(APPROVED_COLS); wb.save(path)
            wb = load_workbook(path); ws = wb.active
            for row_dict in rows_to_write: ws.append([row_dict.get(h, "") for h in APPROVED_COLS])
            wb.save(path)
        try:
            safe_write_with_lock(APPROVED_EXCEL_PATH, writer)
            logging.info(f"ğŸ“¦ (åå°)æ‰¹é‡å†™å…¥ 'å·²é€šè¿‡' {len(rows_to_write)} æ¡æˆåŠŸã€‚")
        except Exception as e: logging.error(f"âš ï¸ (åå°)æ‰¹é‡å†™å…¥ 'å·²é€šè¿‡' å¤±è´¥ï¼Œå°†é‡è¯•: {e}")
approved_batcher = ApprovedBatcher()

def save_for_approved(job: Dict):
    # ã€æ ¸å¿ƒä¿®å¤ã€‘å°† "é‚®ç®±": "email" æ·»åŠ åˆ° update å­—å…¸ä¸­
    info_row_map = {h: h for h in ["ç”¨æˆ·å", "å°çº¢ä¹¦å·", "ä¸»é¡µç½‘å€", "é‚®ç®±", "æœç´¢è¯", "ä¸ªäººç®€ä»‹", "ç²‰ä¸æ•°", "æ€»ç‚¹èµ", "æ ‡è®°æ—¶é—´"]}
    info_row_map.update({
        "ç”¨æˆ·å": "username", 
        "å°çº¢ä¹¦å·": "userid", 
        "ä¸»é¡µç½‘å€": "url", 
        "é‚®ç®±": "email",  # <-- è¿™å°±æ˜¯ç¼ºå¤±çš„å…³é”®æ˜ å°„ï¼
        "ä¸ªäººç®€ä»‹": "bio", 
        "ç²‰ä¸æ•°": "followers", 
        "æ€»ç‚¹èµ": "likes_total", 
        "æ ‡è®°æ—¶é—´": "timestamp", 
        "æœç´¢è¯": "search_term"
    })
    info_data = {k: job.get(v, "") for k, v in info_row_map.items()}
    approved_batcher.add(info_data)

def save_for_review(job: Dict): # ä¿ç•™æ‚¨çš„å®ç°
    row = [job.get("url", ""), job.get("userid", ""), job.get("username", ""), job.get("timestamp", "")]
    def writer(path):
        if not os.path.exists(path):
            wb = Workbook(); ws = wb.active; ws.append(REVIEW_COLS); wb.save(path)
        wb = load_workbook(path); wb.active.append(row); wb.save(path)
    safe_write_with_lock(MANUAL_REVIEW_EXCEL_PATH, writer)

# xhs_user_spider_ai.py -> æ›¿æ¢æ•´ä¸ª save_for_training å‡½æ•°
def save_for_training(job: Dict):
    # ã€æ ¸å¿ƒä¿®å¤ã€‘å°† "é‚®ç®±": "email" æ·»åŠ åˆ° update å­—å…¸ä¸­
    info_row_map = {h: h for h in TRAINING_INFO_COLS}
    info_row_map.update({
        "ç”¨æˆ·å": "username", 
        "å°çº¢ä¹¦å·": "userid", 
        "ä¸»é¡µç½‘å€": "url", 
        "é‚®ç®±": "email", # <-- è¿™é‡Œä¹ŸåŠ ä¸Šï¼Œç¡®ä¿ä¸‡æ— ä¸€å¤±
        "å®¡æ ¸çŠ¶æ€": "status", 
        "AIé¢„æµ‹æ¦‚ç‡": "ai_prob", 
        "AIé¢„æµ‹çŠ¶æ€": "ai_decision", 
        "ä¸ªäººç®€ä»‹": "bio", 
        "ç²‰ä¸æ•°": "followers", 
        "æ€»ç‚¹èµ": "likes_total", 
        "æ ‡è®°æ—¶é—´": "timestamp", 
        "æœç´¢è¯": "search_term"
    })
    info_row = {k: job.get(v, "") for k, v in info_row_map.items()}
    
    # åç»­ç¬”è®°å¤„ç†é€»è¾‘ä¿æŒä¸å˜
    notes_rows = []
    userid = job.get("userid") or job.get("username") or "unknown"
    futures = {i: img_pool.submit(download_and_convert_image, n.get("cover_url", ""), userid) for i, n in enumerate(job.get("notes", [])[:NUM_NOTES_TO_PROCESS]) if n.get("cover_url")}
    covers = {i: fut.result() for i, fut in futures.items()}
    for i, n in enumerate(job.get("notes", [])[:NUM_NOTES_TO_PROCESS]): 
        notes_rows.append([job.get("userid", ""), i + 1, n.get("title", ""), parse_count_chinese(str(n.get("likes", "0"))), covers.get(i, ""), job.get("timestamp", "")])
    
    def writer(path):
        if not os.path.exists(path):
            wb = Workbook()
            ws_info = wb.active
            ws_info.title = INFO_SHEET
            ws_info.append(TRAINING_INFO_COLS)
            ws_notes = wb.create_sheet(NOTES_SHEET)
            ws_notes.append(TRAINING_NOTES_COLS)
            wb.save(path)
        wb = load_workbook(path)
        ws_info = wb[INFO_SHEET] if INFO_SHEET in wb.sheetnames else wb.create_sheet(INFO_SHEET, 0)
        ws_info.append([info_row.get(h, "") for h in TRAINING_INFO_COLS])
        if notes_rows:
            ws_notes = wb[NOTES_SHEET] if NOTES_SHEET in wb.sheetnames else wb.create_sheet(NOTES_SHEET, 1)
            for r in notes_rows: 
                ws_notes.append(r)
        wb.save(path)
    
    safe_write_with_lock(NEW_TRAINING_DATA_EXCEL, writer)

# ã€ä¿®æ”¹ã€‘ç®€åŒ– process_job
def process_job(job: Dict):
    job_id = job.get("job_id")
    if job_id and r.sismember(WAL_DONE_SET, job_id): 
        return
    status = job.get("status")

    # ç»Ÿä¸€è°ƒç”¨æ–°çš„å»é‡å‡½æ•°
    save_to_fetched_list(username=job.get("username"), userid=job.get("userid"))

    if status == "ç¬¦åˆ":
        try: save_for_approved(job)
        except Exception as e: logging.error(f"âŒ [åå°] ä¿å­˜ 'å·²é€šè¿‡' æ•°æ®å¤±è´¥: {e}")
    if status == "äººå·¥å®¡æ ¸":
        try: save_for_review(job); logging.info(f"ğŸ“‹ (åå°)å·²ä¿å­˜è‡³å¾…å¤å®¡: {job.get('username') or job.get('userid')}")
        except Exception as e: logging.error(f"âŒ [åå°] ä¿å­˜ 'å¾…å¤å®¡' æ•°æ®å¤±è´¥: {e}")
        
    save_history_enabled = r.get(SAVE_HISTORY_ENABLED_KEY) == "1"
    if status == "ç¬¦åˆ" or (status == "ä¸ç¬¦åˆ" and save_history_enabled):
        try: save_for_training(job)
        except Exception as e: logging.warning(f"âš ï¸ [åå°] å†™å…¥è®­ç»ƒæ•°æ®å¤±è´¥: {e}")
        
    if job_id: 
        r.sadd(WAL_DONE_SET, job_id)

def consume_jobs(): # ä¿ç•™æ‚¨çš„å®ç°
    logging.info("âœ… åå°æ¶ˆè´¹çº¿ç¨‹å·²å¯åŠ¨...")
    while not shutdown_event.is_set():
        try:
            job = job_q.get(timeout=1)
            process_job(job)
            job_q.task_done()
        except queue.Empty:
            approved_batcher.flush()
            continue
        except Exception as e: logging.error(f"âŒ å¤„ç†åå°ä»»åŠ¡æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}"); time.sleep(2)

# ========== AI é¢„æµ‹æ ¸å¿ƒå‡½æ•° (æ¥è‡ªæ‚¨çš„ v5.5) ==========
def run_ai_prediction(blogger_data: Dict) -> (float, str, str):
    if not AI_ENABLED_BY_FILE: return 0.0, "error", "AIæ¨¡å‹æœªå¯ç”¨æˆ–åŠ è½½å¤±è´¥"
    try:
        text_dim=TEXT_EMBEDDING_MODEL.get_sentence_embedding_dimension()or 768;image_dim=IMAGE_EMBEDDING_MODEL.get_sentence_embedding_dimension()or 512;bio=str(blogger_data.get('bio',''));notes_data=blogger_data.get('notes',[]);found_email=extract_and_normalize_email(bio);has_email=1 if found_email else 0;followers=parse_count_chinese(str(blogger_data.get('followers',0)));total_likes=parse_count_chinese(str(blogger_data.get('likes_total',0)));avg_likes_per_fan=total_likes/(followers+1)if followers>0 else 0;s_ratio,d_ratio=0,0
        if notes_data:likes_list=[parse_count_chinese(str(n.get('likes','0')))for n in notes_data[:NUM_NOTES_TO_PROCESS]];s_ratio=sum(1 for l in likes_list if l<10)/len(likes_list)if likes_list else 0;d_ratio=sum(1 for l in likes_list if 10<=l<100)/len(likes_list)if likes_list else 0
        bio_vec=TEXT_EMBEDDING_MODEL.encode([bio],convert_to_tensor=True,device=AI_DEVICE);titles=[n.get('title','')for n in notes_data[:NUM_NOTES_TO_PROCESS]];title_vec=TEXT_EMBEDDING_MODEL.encode(titles,convert_to_tensor=True,device=AI_DEVICE).mean(axis=0,keepdim=True)if titles else torch.zeros((1,text_dim),device=AI_DEVICE);image_urls=[note.get('cover_url')for note in notes_data if note.get('cover_url')];image_vec=torch.zeros((1,image_dim),device=AI_DEVICE)
        if image_urls:
            pil_images=[]
            for url in image_urls[:NUM_NOTES_TO_PROCESS]:
                if url:
                    try:pil_images.append(Image.open(io.BytesIO(http.get(url,timeout=5).content)).convert("RGB"))
                    except:pass
            if pil_images:image_vec=IMAGE_EMBEDDING_MODEL.encode(pil_images,convert_to_tensor=True,device=AI_DEVICE).mean(axis=0,keepdim=True)
        numeric_features=np.array([[has_email,followers,total_likes,avg_likes_per_fan,s_ratio,d_ratio]],dtype=np.float32);full_feature_np=np.concatenate([numeric_features,bio_vec.cpu().numpy(),title_vec.cpu().numpy(),image_vec.cpu().numpy()],axis=1)
        if full_feature_np.shape[1]<len(AI_SCALER.feature_names_in_):padded_arr=np.zeros((1,len(AI_SCALER.feature_names_in_)));padded_arr[:,:full_feature_np.shape[1]]=full_feature_np;full_feature_np=padded_arr
        scaled_features=AI_SCALER.transform(pd.DataFrame(full_feature_np,columns=AI_SCALER.feature_names_in_))
        with torch.no_grad():probability=AI_MODEL(torch.tensor(scaled_features,dtype=torch.float32).to(AI_DEVICE)).item()
        if probability>AI_ACCEPT_THRESHOLD:decision="ç¬¦åˆ";reason=f"æ¨¡å‹æ¦‚ç‡ {probability:.2f} > {AI_ACCEPT_THRESHOLD}"
        elif probability>=AI_REJECT_THRESHOLD:decision="äººå·¥å®¡æ ¸";reason=f"æ¨¡å‹æ¦‚ç‡ {probability:.2f} åœ¨ [{AI_REJECT_THRESHOLD}, {AI_ACCEPT_THRESHOLD}] ä¹‹é—´"
        else:decision="ä¸ç¬¦åˆ";reason=f"æ¨¡å‹æ¦‚ç‡ {probability:.2f} < {AI_REJECT_THRESHOLD}"
        return round(probability,4),decision,reason
    except Exception as e:
        logging.error(f"âŒ AI Prediction Error: {e}");return 0.0,"error",str(e)
# (æ‰¾åˆ° ai_extract_email_by_model å‡½æ•°ï¼Œå¹¶ç”¨ä¸‹é¢çš„ä»£ç å®Œå…¨æ›¿æ¢å®ƒ)

def ai_extract_email_by_model(blogger_data: Dict) -> Optional[str]:
    """
    ã€V4 - æœ€ç»ˆç‰ˆAIé‚®ç®±æå–ã€‘
    1. ã€æ ¸å¿ƒä¿®æ­£ã€‘ç›´æ¥åœ¨æœ€åŸå§‹çš„ç®€ä»‹æ–‡æœ¬ä¸Šæ“ä½œï¼Œä¿ç•™æ‰€æœ‰ç‰¹æ®Šå­—ç¬¦å’Œé¢œæ–‡å­—ã€‚
    2. ä½¿ç”¨ä¸€ä¸ªæå…¶å®½å®¹çš„æ­£åˆ™è¡¨è¾¾å¼æ¥æ•æ‰ä»»ä½•å½¢å¼çš„'xxx@yyy'å€™é€‰è€…ï¼Œæ— è®ºå­—ç¬¦å¤šæ€ªå¼‚ã€‚
    3. AIæ¨¡å‹é€šè¿‡åœ¨åŸå§‹æ–‡æœ¬ä¸Šè¿›è¡Œâ€œæŒ–æ´â€å‡è®¾æ£€éªŒï¼Œæ¥åˆ¤æ–­å“ªä¸ªâ€œæ€ªå¼‚â€çš„å­—ç¬¦ä¸²æ˜¯å…³é”®ä¿¡æ¯ã€‚
    4. è¿”å›çš„æ˜¯åŸå§‹å­—ç¬¦ä¸²ï¼Œäº¤ç”±åç»­é€»è¾‘å¤„ç†ã€‚
    """
    if not AI_ENABLED_BY_FILE:
        return None

    original_bio = blogger_data.get("bio", "")
    if not original_bio:
        return None

    try:
        # 1. ã€æ ¸å¿ƒä¿®æ­£ã€‘ç›´æ¥åœ¨åŸå§‹bioä¸Šå¯»æ‰¾å€™é€‰è€…
        # è¿™ä¸ªæ­£åˆ™æå…¶å®½å®¹ï¼š
        # - `[^\s@]+` åŒ¹é…ä»»ä½•éç©ºç™½ã€é@çš„å­—ç¬¦ä¸€æ¬¡æˆ–å¤šæ¬¡ (ç”¨æˆ·åéƒ¨åˆ†)
        # - `@{1}` åŒ¹é…ä¸€ä¸ª@ç¬¦å·
        # - `[^\s@]+` å†æ¬¡åŒ¹é…ä»»ä½•éç©ºç™½ã€é@çš„å­—ç¬¦ (åŸŸåéƒ¨åˆ†)
        # å®ƒå¯ä»¥å®Œç¾åŒ¹é… "ğŸ“¢517656306@ğ“ºğ“º.ğ“¬ğ“¸ğ“¶"
        candidates = re.findall(r'[^\s@]+@{1}[^\s@]+', original_bio)

        # å¤‡ç”¨ç­–ç•¥ï¼šå¦‚æœæ‰¾ä¸åˆ°@ç¬¦å·ï¼Œæˆ‘ä»¬ä¾ç„¶åœ¨â€œå‡€åŒ–åâ€çš„æ–‡æœ¬é‡Œå¯»æ‰¾é•¿æ•°å­—ä¸²
        if not candidates:
            normalized_bio_for_numeric = original_bio.lower()
            for non_standard, standard in EMAIL_MAPPING_DICT.items():
                normalized_bio_for_numeric = normalized_bio_for_numeric.replace(non_standard, standard)
            
            numeric_candidates = re.findall(r'[1-9][0-9]{4,}', normalized_bio_for_numeric)
            if numeric_candidates:
                longest_numeric = max(numeric_candidates, key=len)
                candidates.append(longest_numeric)

        if not candidates:
            logging.info("ğŸ¤– AIé‚®ç®±è¯†åˆ«: åœ¨åŸå§‹æ–‡æœ¬ä¸­æœªæ‰¾åˆ°ä»»ä½•å€™é€‰è”ç³»æ–¹å¼ã€‚")
            return None

        # 2. å®šä¹‰å†…è”çš„AIé¢„æµ‹å‡½æ•° (å®ƒå°†åœ¨åŸå§‹æ–‡æœ¬ä¸Šå·¥ä½œ)
        def get_prob_for_bio(temp_bio: str) -> float:
            temp_data = blogger_data.copy()
            temp_data['bio'] = temp_bio
            text_dim=TEXT_EMBEDDING_MODEL.get_sentence_embedding_dimension()or 768;image_dim=IMAGE_EMBEDDING_MODEL.get_sentence_embedding_dimension()or 512;has_email=1;followers=parse_count_chinese(str(temp_data.get('followers',0)));total_likes=parse_count_chinese(str(temp_data.get('likes_total',0)));avg_likes_per_fan=total_likes/(followers+1)if followers>0 else 0;s_ratio,d_ratio=0,0;notes_data=temp_data.get('notes',[]);
            if notes_data:likes_list=[parse_count_chinese(str(n.get('likes','0')))for n in notes_data[:NUM_NOTES_TO_PROCESS]];s_ratio=sum(1 for l in likes_list if l<10)/len(likes_list)if likes_list else 0;d_ratio=sum(1 for l in likes_list if 10<=l<100)/len(likes_list)if likes_list else 0
            bio_vec=TEXT_EMBEDDING_MODEL.encode([temp_data['bio']],convert_to_tensor=True,device=AI_DEVICE)
            titles=[n.get('title','')for n in notes_data[:NUM_NOTES_TO_PROCESS]];title_vec=TEXT_EMBEDDING_MODEL.encode(titles,convert_to_tensor=True,device=AI_DEVICE).mean(axis=0,keepdim=True)if titles else torch.zeros((1,text_dim),device=AI_DEVICE);image_urls=[note.get('cover_url')for note in notes_data if note.get('cover_url')];image_vec=torch.zeros((1,image_dim),device=AI_DEVICE)
            numeric_features=np.array([[has_email,followers,total_likes,avg_likes_per_fan,s_ratio,d_ratio]],dtype=np.float32);full_feature_np=np.concatenate([numeric_features,bio_vec.cpu().numpy(),title_vec.cpu().numpy(),image_vec.cpu().numpy()],axis=1)
            if full_feature_np.shape[1]<len(AI_SCALER.feature_names_in_):padded_arr=np.zeros((1,len(AI_SCALER.feature_names_in_)));padded_arr[:,:full_feature_np.shape[1]]=full_feature_np;full_feature_np=padded_arr
            scaled_features=AI_SCALER.transform(pd.DataFrame(full_feature_np,columns=AI_SCALER.feature_names_in_))
            with torch.no_grad():return AI_MODEL(torch.tensor(scaled_features,dtype=torch.float32).to(AI_DEVICE)).item()

        # 3. è®©AIåœ¨æœ€åŸå§‹çš„æ–‡æœ¬ä¸Šè¿›è¡Œè¯„ä¼°
        base_probability = get_prob_for_bio(original_bio)
        best_candidate = None
        max_prob_drop = -1

        for cand in set(candidates):
            bio_without_cand = original_bio.replace(cand, "")
            prob_without_cand = get_prob_for_bio(bio_without_cand)
            prob_drop = base_probability - prob_without_cand
            
            if prob_drop > max_prob_drop:
                max_prob_drop = prob_drop
                best_candidate = cand
        
        # 4. æ™ºèƒ½å†³ç­–ä¸è¿”å›
        if max_prob_drop > 0.03:
            final_contact_info = best_candidate
            logging.info(f"ğŸ¤– AIé‚®ç®±è¯†åˆ«: æ‰¾åˆ°å…³é”®ä¿¡æ¯ '{final_contact_info}' (ç§»é™¤åæ¦‚ç‡ä¸‹é™ {max_prob_drop:.2%})")
            
            # ã€é‡è¦ã€‘è¿”å›çš„æ˜¯åŸå§‹å­—ç¬¦ä¸²ï¼Œæˆ‘ä»¬ç›¸ä¿¡åç»­çš„æ ‡å‡†å‡½æ•°èƒ½å¤„ç†å®ƒ
            # ä½†æˆ‘ä»¬å¯ä»¥åœ¨è¿™é‡Œåšä¸€ä¸ªæœ€ç»ˆçš„â€œç¿»è¯‘â€å°è¯•ï¼Œè¿”å›ä¸€ä¸ªæ›´å¹²å‡€çš„ç‰ˆæœ¬
            normalized_result = final_contact_info.lower()
            for non_standard, standard in EMAIL_MAPPING_DICT.items():
                normalized_result = normalized_result.replace(non_standard, standard)

            # å¦‚æœç¿»è¯‘åæ˜¯çº¯æ•°å­—ï¼Œè¡¥å…¨qq.com
            if normalized_result.isdigit():
                return f"{normalized_result}@qq.com"

            # å¦åˆ™è¿”å›ç¿»è¯‘åçš„ç»“æœ
            return normalized_result
        else:
            logging.info(f"ğŸ¤– AIé‚®ç®±è¯†åˆ«: æœªæ‰¾åˆ°å…³é”®è”ç³»æ–¹å¼ (æœ€å¤§æ¦‚ç‡å˜åŒ– {max_prob_drop:.2%})")
            return None

    except Exception as e:
        logging.error(f"âŒ AIé‚®ç®±è¯†åˆ«æ—¶å‡ºé”™: {e}")
        return None

# ã€é‡æ„ã€‘æ–°çš„å¥å£®çš„æ•°æ®åŠ è½½å‡½æ•°
def load_fetched_list_to_redis():
    """v5.6: ä½¿ç”¨Pandasä»ExcelåŠ è½½æ•°æ®åˆ°Redisï¼Œæ›´å¥å£®ã€‚"""
    
    # 1. ä¼˜å…ˆä»æ–°ç‰ˆç‹¬ç«‹æ–‡ä»¶åŠ è½½
    if os.path.exists(FETCHED_USERNAMES_FILE):
        df_users = pd.read_excel(FETCHED_USERNAMES_FILE)
        if "ç”¨æˆ·å" in df_users.columns:
            users_to_add = df_users["ç”¨æˆ·å"].dropna().astype(str).tolist()
            if users_to_add: r.sadd(USERNAMES_SET_KEY, *users_to_add)
            logging.info(f"âœ… ä» {FETCHED_USERNAMES_FILE} åŠ è½½ {len(users_to_add)} ä¸ªç”¨æˆ·ååˆ°Redisã€‚")

    if os.path.exists(FETCHED_USERIDS_FILE):
        df_ids = pd.read_excel(FETCHED_USERIDS_FILE)
        if "å°çº¢ä¹¦å·" in df_ids.columns:
            ids_to_add = df_ids["å°çº¢ä¹¦å·"].dropna().astype(str).tolist()
            if ids_to_add: r.sadd(USERIDS_SET_KEY, *ids_to_add)
            logging.info(f"âœ… ä» {FETCHED_USERIDS_FILE} åŠ è½½ {len(ids_to_add)} ä¸ªç”¨æˆ·IDåˆ°Redisã€‚")

    # 2. å…¼å®¹å¹¶è¿ç§»æ—§ç‰ˆåˆå¹¶æ–‡ä»¶
    if os.path.exists(FETCHED_USER_LIST_PATH):
        logging.warning(f"æ£€æµ‹åˆ°æ—§ç‰ˆæ–‡ä»¶ '{FETCHED_USER_LIST_PATH}'ï¼Œå°†è¿›è¡Œä¸€æ¬¡æ€§è¿ç§»ã€‚")
        try:
            df = pd.read_excel(FETCHED_USER_LIST_PATH, sheet_name=None)
            users_migrated, ids_migrated = 0, 0
            
            if "çˆ¬å–ç”¨æˆ·å" in df:
                col_name = "çˆ¬å–ç”¨æˆ·å" if "çˆ¬å–ç”¨æˆ·å" in df["çˆ¬å–ç”¨æˆ·å"].columns else ("ç”¨æˆ·å" if "ç”¨æˆ·å" in df["çˆ¬å–ç”¨æˆ·å"].columns else None)
                if col_name:
                    users_to_add = df["çˆ¬å–ç”¨æˆ·å"][col_name].dropna().astype(str).tolist()
                    if users_to_add:
                        r.sadd(USERNAMES_SET_KEY, *users_to_add)
                        _perform_save_to_excel(FETCHED_USERNAMES_FILE, users_to_add, "ç”¨æˆ·å")
                        users_migrated = len(users_to_add)

            if "å°çº¢ä¹¦å·" in df:
                if "å°çº¢ä¹¦å·" in df["å°çº¢ä¹¦å·"].columns:
                    ids_to_add = df["å°çº¢ä¹¦å·"]["å°çº¢ä¹¦å·"].dropna().astype(str).tolist()
                    if ids_to_add:
                        r.sadd(USERIDS_SET_KEY, *ids_to_add)
                        _perform_save_to_excel(FETCHED_USERIDS_FILE, ids_to_add, "å°çº¢ä¹¦å·")
                        ids_migrated = len(ids_to_add)
            
            logging.info(f"âœ… æ—§æ–‡ä»¶è¿ç§»å®Œæˆ: {users_migrated} ç”¨æˆ·å, {ids_migrated} IDã€‚å»ºè®®è¿ç§»ååˆ é™¤æ—§æ–‡ä»¶ã€‚")
            # os.rename(FETCHED_USER_LIST_PATH, FETCHED_USER_LIST_PATH + ".bak") # å¯é€‰ï¼šè‡ªåŠ¨é‡å‘½å
        except Exception as e:
            logging.error(f"âŒ è¿ç§»æ—§æ–‡ä»¶ '{FETCHED_USER_LIST_PATH}' å¤±è´¥: {e}")


# ========== API è·¯ç”± (ä¿ç•™æ‚¨çš„ v5.5 ç»“æ„) ==========
@app.route("/usernames", methods=["GET"])
def get_usernames(): return jsonify(list(r.smembers(USERNAMES_SET_KEY)))
@app.route("/userids", methods=["GET"])
def get_userids(): return jsonify(list(r.smembers(USERIDS_SET_KEY)))
# (åœ¨ @app.route("/mark_data", methods=["POST"]) è¿™ä¸€è¡Œçš„ä¸Šæ–¹ï¼Œç²˜è´´ä¸‹é¢çš„æ–°å‡½æ•°)

def _process_mark_data(data: Dict):
    """
    ã€æ–°ã€‘è¿™æ˜¯ mark_data çš„æ ¸å¿ƒé€»è¾‘å¤„ç†å‡½æ•°ï¼Œä¸ä¾èµ–äºWebè¯·æ±‚ä¸Šä¸‹æ–‡ã€‚
    å®ƒæ¥æ”¶ä¸€ä¸ªæ•°æ®å­—å…¸ï¼Œå¹¶å®Œæˆæ‰€æœ‰åç»­å¤„ç†ã€‚
    """
    username, userid = (data.get("username") or "").strip(), (data.get("userid") or "").strip()
    final_decision = (data.get("status") or "").strip()

    if not final_decision:
        ai_prob, ai_decision, _ = run_ai_prediction(data)
        final_decision = ai_decision
        data['status'] = final_decision; data['ai_prob'] = ai_prob; data['ai_decision'] = ai_decision
    else:
        ai_prob, ai_decision, _ = run_ai_prediction(data)
        data['ai_prob'] = ai_prob; data['ai_decision'] = ai_decision

    if not final_decision or (not username and not userid):
        logging.warning(f"[_process_mark_data] ç¼ºå°‘å…³é”®å‚æ•°ï¼Œä»»åŠ¡ä¸­æ­¢ã€‚Data: {data}")
        return {"status": "error", "message": "ç¼ºå°‘å…³é”®å‚æ•°"}

    # â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼ æ ¸å¿ƒä¿®å¤ç‚¹ â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼
    with mark_data_lock:
        is_uname_member = r.sismember(USERNAMES_SET_KEY, username) if username else False
        is_uid_member = r.sismember(USERIDS_SET_KEY, str(userid)) if userid else False
        is_rereview = data.get("is_rereview", False) # <-- æ–°å¢ï¼šè·å–å‰ç«¯æ ‡å¿—

        # ä¿®æ”¹é€»è¾‘ï¼šåªæœ‰å½“å®ƒæ˜¯é‡å¤çš„ã€ä¸æ˜¯äººå·¥å®¡æ ¸ã€å¹¶ä¸”ä¸æ˜¯å¤å®¡ä»»åŠ¡æ—¶ï¼Œæ‰åˆ¤å®šä¸ºé‡å¤
        if (is_uname_member or is_uid_member) and final_decision != "äººå·¥å®¡æ ¸" and not is_rereview:
            logging.warning(f"å‘ç°é‡å¤æäº¤ï¼ˆéå¤å®¡æ¨¡å¼ï¼‰ï¼Œå·²è·³è¿‡: user=({username}|{userid})")
            return {"status": "duplicated"}

        # å¯¹äºâ€œç¬¦åˆâ€æˆ–â€œä¸ç¬¦åˆâ€çš„å†³å®šï¼ˆæ— è®ºæ˜¯é¦–æ¬¡è¿˜æ˜¯å¤å®¡ï¼‰ï¼Œéƒ½ç¡®ä¿å…¶å­˜åœ¨äºRedisä¸­
        if final_decision in ["ç¬¦åˆ", "ä¸ç¬¦åˆ"]:
            if username: r.sadd(USERNAMES_SET_KEY, username)
            if userid: r.sadd(USERIDS_SET_KEY, str(userid))
    # â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–² æ ¸å¿ƒä¿®å¤ç‚¹ â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²

    # V6 æœ€ç»ˆç‰ˆé‚®ç®±å¤„ç†é€»è¾‘
    final_email = ""
    bio_text = data.get("bio", "")
    frontend_email = data.get("email")
    if isinstance(frontend_email, str) and frontend_email.strip():
        final_email = frontend_email.strip(); logging.info("ğŸ“§ é‚®ç®±æ¥æº: å‰ç«¯æ‰‹åŠ¨å¡«å†™ã€‚")
    if not final_email and bio_text:
        final_email = extract_and_normalize_email(bio_text) or "";
        if final_email: logging.info("ğŸ“§ é‚®ç®±æ¥æº: æ ‡å‡†å‡½æ•°è¯†åˆ«ã€‚")
    if not final_email and bio_text:
        final_email = ai_extract_email_by_model(data) or "";
        if final_email: logging.info("ğŸ“§ é‚®ç®±æ¥æº: AIæ¨¡å‹è¯†åˆ«ã€‚")
    data['email'] = final_email

    ts = now_str()
    job_id = f"{userid or username}:{int(time.time()*1000)}:{final_decision}"
    job = {**data, "job_id": job_id, "timestamp": ts, "status": final_decision}
    
    try:
        with open(WAL_FILE, "a", encoding="utf-8") as f: f.write(json.dumps(job, ensure_ascii=False) + "\n")
        job_q.put(job, timeout=0.1)
    except queue.Full:
        logging.error("æœåŠ¡ç¹å¿™ï¼Œåå°ä»»åŠ¡é˜Ÿåˆ—å·²æ»¡ï¼")
        return {"status": "error", "message": "æœåŠ¡ç¹å¿™ï¼Œé˜Ÿåˆ—å·²æ»¡"}
    except Exception as e:
        logging.error(f"å†™å…¥WALæˆ–å…¥é˜Ÿæ—¶å‡ºé”™: {e}")
        return {"status": "error", "message": "å†…éƒ¨é”™è¯¯"}
    
    logging.info(f"    -> ä»»åŠ¡å·²å…¥é˜Ÿ: {final_decision} (AIé¢„æµ‹: {ai_decision}, P={ai_prob:.4f}), user=({username}|{userid})")
    update_and_print_daily_stats(final_decision)
    return {"status": "ok", "message": "ä»»åŠ¡å·²æ¥æ”¶"}


@app.route("/mark_data", methods=["POST"])
def mark_data():
    data = request.get_json(silent=True) or {}
    result = _process_mark_data(data)
    
    status_code = 200
    if result.get("status") == "error":
        if "ç¼ºå°‘å…³é”®å‚æ•°" in result.get("message", ""): status_code = 400
        elif "é˜Ÿåˆ—å·²æ»¡" in result.get("message", ""): status_code = 503
        else: status_code = 500
        
    return jsonify(result), status_code

@app.route("/ai/decide", methods=["POST"])
def ai_decide():
    data = request.get_json(silent=True)
    if not data:
        return jsonify({"status": "error", "message": "No data provided"}), 400

    p, dec, why = run_ai_prediction(data)
    data['status'] = dec
    
    # ã€æ ¸å¿ƒä¿®å¤ã€‘ç›´æ¥åœ¨åå°çº¿ç¨‹ä¸­è°ƒç”¨æ ¸å¿ƒé€»è¾‘å‡½æ•°ï¼Œä¸å†é€šè¿‡HTTPè¯·æ±‚ç»•åœˆ
    threading.Thread(target=_process_mark_data, args=(data,)).start()
    
    return jsonify({"decision": dec, "reason": why, "p_base": p})
    data = request.get_json(silent=True)
    if not data:
        return jsonify({"status": "error", "message": "No data provided"}), 400

    p, dec, why = run_ai_prediction(data)
    data['status'] = dec
    
    # ã€æ ¸å¿ƒä¿®å¤ã€‘ç›´æ¥åœ¨åå°çº¿ç¨‹ä¸­è°ƒç”¨æ ¸å¿ƒé€»è¾‘å‡½æ•°ï¼Œä¸å†é€šè¿‡HTTPè¯·æ±‚ç»•åœˆ
    threading.Thread(target=_process_mark_data, args=(data,)).start()
    
    return jsonify({"decision": dec, "reason": why, "p_base": p})
    data = request.get_json(silent=True) or {}
    result = _process_mark_data(data)
    
    status_code = 200
    if result.get("status") == "error":
        if "ç¼ºå°‘å…³é”®å‚æ•°" in result.get("message", ""): status_code = 400
        elif "é˜Ÿåˆ—å·²æ»¡" in result.get("message", ""): status_code = 503
        else: status_code = 500
        
    return jsonify(result), status_code
# ... (ä¿ç•™æ‚¨ v5.5 çš„å…¶ä»–è·¯ç”±)
@app.route("/ai/settings", methods=["POST"])
def ai_settings(): enabled=bool(request.get_json(silent=True).get("enabled",False));r.set(AI_ENABLED_KEY,"1" if enabled else "0");logging.info(f"âš™ï¸ AIè‡ªåŠ¨å®¡æ ¸å·² {'å¼€å¯' if enabled else 'å…³é—­'}");return jsonify({"status":"ok","ai_enabled":enabled})
@app.route("/settings/save_history", methods=["POST"])
def save_history_settings(): enabled=bool(request.get_json(silent=True).get("enabled",False));r.set(SAVE_HISTORY_ENABLED_KEY,"1" if enabled else "0");logging.info(f"âš™ï¸ ä¿å­˜â€œä¸ç¬¦åˆâ€çš„è®­ç»ƒæ•°æ®å·² {'å¼€å¯' if enabled else 'å…³é—­'}");return jsonify({"status":"ok","save_history_enabled":enabled})
@app.route("/ai/suggest", methods=["POST"])
def ai_predict_only(): blogger_data = request.get_json(silent=True); p, dec, why = run_ai_prediction(blogger_data); return jsonify({"decision": dec, "reason": why, "p_base": p})
@app.route("/get_review_list", methods=["GET"])
def get_review_list():
    if not os.path.exists(MANUAL_REVIEW_EXCEL_PATH): 
        return jsonify([])
    try:
        def reader(path):
            wb = load_workbook(path); ws = wb.active
            if ws.max_row < 2: return [] 
            urls = [row[0] for row in ws.iter_rows(min_row=2, max_row=ws.max_row, values_only=True) if row and row[0]]
            if urls: ws.delete_rows(2, len(urls)); wb.save(path)
            return urls
        urls = safe_write_with_lock(MANUAL_REVIEW_EXCEL_PATH, reader) or []
        logging.info(f"âœ… æä¾›äº† {len(urls)} ä¸ªå¾…å¤å®¡URLï¼Œå¹¶å·²æ¸…ç©ºåˆ—è¡¨ã€‚")
        return jsonify(urls)
    except Exception as e: 
        logging.error(f"âŒ è¯»å– '{MANUAL_REVIEW_EXCEL_PATH}' å¤±è´¥: {e}")
        return jsonify({"status": "error", "msg": str(e)}), 500

def read_sheet_as_dicts(xlsx_path: str, sheet_name: str) -> List[Dict]:
    if not os.path.exists(xlsx_path): return []
    wb=load_workbook(xlsx_path,read_only=True);ws=wb[sheet_name]
    if ws.max_row<2:return[]
    rows=list(ws.iter_rows(values_only=True));headers=[str(h or "").strip()for h in rows[0]];return[dict(zip(headers,r))for r in rows[1:]]
def _filter_delta_rows(rows: List[Dict], min_dt: Optional[datetime]) -> List[Dict]:
    return [d for d in rows if(dt:=parse_dt(str(d.get("æ ‡è®°æ—¶é—´",""))))and dt>min_dt] if min_dt else rows
@app.route("/export_delta", methods=["GET"])
def export_delta():
    dataset=(request.args.get("dataset")or"approved").lower()
    curA_dt=parse_dt(r.get(CUR_APPROVED))if r.exists(CUR_APPROVED)else None
    wb=Workbook();wb.remove(wb.active)
    if dataset in("approved","both"):
        ws_info=wb.create_sheet("å·²é€šè¿‡-åšä¸»ä¿¡æ¯");ws_info.append(APPROVED_COLS)
        if os.path.exists(APPROVED_EXCEL_PATH):
            all_info=read_sheet_as_dicts(APPROVED_EXCEL_PATH,INFO_SHEET)
            for d in _filter_delta_rows(all_info,curA_dt):ws_info.append([d.get(k,"")for k in APPROVED_COLS])
    ts_name=datetime.now().strftime("%Y%m%d_%H%M%S");out_name=f"{DELTA_PREFIX}{ts_name}.xlsx";wb.save(out_name);nowS=now_str()
    if dataset in("approved","both"):r.set(CUR_APPROVED,nowS)
    logging.info(f"ğŸ“¤ å¢é‡å¯¼å‡ºå®Œæˆ: {out_name}, æ¸¸æ ‡å·²æ›´æ–°è‡³ {nowS}");return send_file(out_name,as_attachment=True,download_name=out_name)
@app.route("/rebuild_sets", methods=["POST"])
def rebuild_sets():
    try:
        logging.info("ğŸ” å¼€å§‹ä»æ‰€æœ‰Excelæ–‡ä»¶é‡å»ºRediså»é‡é›†åˆ..."); r.delete(USERNAMES_SET_KEY); r.delete(USERIDS_SET_KEY)
        for path in [APPROVED_EXCEL_PATH, NEW_TRAINING_DATA_EXCEL, MANUAL_REVIEW_EXCEL_PATH]:
            if not os.path.exists(path): continue
            for row in read_sheet_as_dicts(path, INFO_SHEET if path != MANUAL_REVIEW_EXCEL_PATH else "Sheet"):
                if u := (row.get("ç”¨æˆ·å") or row.get("URL") or "").strip(): r.sadd(USERNAMES_SET_KEY, u)
                if i := (row.get("å°çº¢ä¹¦å·") or "").strip(): r.sadd(USERIDS_SET_KEY, str(i))
        load_fetched_list_to_redis()
        final_user_count,final_id_count=r.scard(USERNAMES_SET_KEY),r.scard(USERIDS_SET_KEY)
        logging.info(f"âœ… Rediså»é‡é›†åˆå·²é‡å»ºå®Œæˆ: {final_user_count} ç”¨æˆ·å, {final_id_count} å°çº¢ä¹¦å·ã€‚")
        return jsonify({"status":"ok","usernames":final_user_count,"userids":final_id_count})
    except Exception as e:return jsonify({"status":"error","msg":str(e)}),500
@app.route('/dashboard', methods=['GET'])
def dashboard_page():
    try:
        with open('dashboard.html','r',encoding='utf-8')as f:return render_template_string(f.read())
    except FileNotFoundError:return"Error: dashboard.html not found.",404
@app.route("/dashboard_stats", methods=["GET"])
def dashboard_stats():
    try:
        today_str=get_statistical_date_str();approved_today=int(r.get(f"daily_stats:approved:{today_str}")or 0);rejected_today=int(r.get(f"daily_stats:rejected:{today_str}")or 0);yesterday_dt=datetime.now()-timedelta(days=1);yesterday_str=get_statistical_date_str(yesterday_dt);approved_yesterday=int(r.get(f"daily_stats:approved:{yesterday_str}")or 0);rejected_yesterday=int(r.get(f"daily_stats:rejected:{yesterday_str}")or 0);pending_review_count=0
        if os.path.exists(MANUAL_REVIEW_EXCEL_PATH):
            def reader(path): return max(0,load_workbook(path).active.max_row-1)
            pending_review_count = safe_write_with_lock(MANUAL_REVIEW_EXCEL_PATH, reader) or 0
        return jsonify({"today":{"approved":approved_today,"rejected":rejected_today,"total":approved_today+rejected_today,},"yesterday":{"total":approved_yesterday+rejected_yesterday,},"pending_review":pending_review_count})
    except Exception as e:return jsonify({"status":"error","msg":str(e)}),500
@app.route('/open_folder', methods=['POST'])
def open_folder_route():
    key=(request.get_json()or{}).get('key');FILE_MAP={"approved":APPROVED_EXCEL_PATH,"review":MANUAL_REVIEW_EXCEL_PATH,"training":NEW_TRAINING_DATA_EXCEL,"output":os.getcwd()};file_path=FILE_MAP.get(key)
    if not file_path:return jsonify({"status":"error","message":"Invalid file key"}),400
    try:
        folder_path=os.path.dirname(os.path.abspath(file_path))if os.path.isfile(file_path)else os.path.abspath(file_path)
        if not os.path.exists(folder_path):os.makedirs(folder_path,exist_ok=True)
        system=platform.system()
        if system=="Windows":os.startfile(folder_path)
        elif system=="Darwin":subprocess.run(["open",folder_path])
        else:subprocess.run(["xdg-open",folder_path])
        logging.info(f"ğŸ“‚ å·²è¯·æ±‚æ‰“å¼€æ–‡ä»¶å¤¹: {folder_path}");return jsonify({"status":"ok","path":folder_path})
    except Exception as e:return jsonify({"status":"error","message":str(e)}),500
@app.route('/download_file', methods=['GET'])
def download_file_route():
    key=request.args.get('key');FILE_MAP={"approved":APPROVED_EXCEL_PATH,"review":MANUAL_REVIEW_EXCEL_PATH,"training":NEW_TRAINING_DATA_EXCEL};file_path=FILE_MAP.get(key)
    if not file_path:return"Invalid file key",400
    if not os.path.exists(file_path):
        if key == 'approved': wb=Workbook();ws=wb.active;ws.title=INFO_SHEET;ws.append(APPROVED_COLS);wb.save(file_path)
        elif key == 'review': wb=Workbook();ws=wb.active;ws.append(REVIEW_COLS);wb.save(file_path)
        elif key == 'training': wb=Workbook();ws_info=wb.active;ws_info.title=INFO_SHEET;ws_info.append(TRAINING_INFO_COLS);ws_notes=wb.create_sheet(NOTES_SHEET);ws_notes.append(TRAINING_NOTES_COLS);wb.save(file_path)
    return send_file(file_path,as_attachment=True)

# ã€ä¿®æ”¹ã€‘/touch_user è·¯ç”±ï¼Œç»Ÿä¸€è°ƒç”¨æ–°å‡½æ•°
@app.route("/touch_user", methods=["POST"])
def touch_user():
    data = request.get_json(silent=True) or {}
    username = (data.get("username") or "").strip()
    userid   = (data.get("userid") or "").strip()
    if not username and not userid:
        return jsonify({"status":"error","message":"missing username/userid"}), 400
    try:
        # ç»Ÿä¸€è°ƒç”¨æ–°çš„ã€é«˜æ•ˆçš„å‡½æ•°ï¼Œå®ƒä¼šå¤„ç†Rediså’Œåå°Excelå†™å…¥
        save_to_fetched_list(username=username, userid=userid)
        return jsonify({"status":"ok"})
    except Exception as e:
        logging.error(f"touch_user error: {e}")
        return jsonify({"status":"error","message":str(e)}), 500

# ========== å¯åŠ¨ä¸é€€å‡º (æ¥è‡ªæ‚¨çš„ v5.5) ==========
def graceful_shutdown(*args, **kwargs):
    logging.info("\nâ¹ï¸ æ­£åœ¨ä¼˜é›…é€€å‡ºï¼Œè¯·ç¨å€™...")
    shutdown_event.set()
    logging.info("   - ç­‰å¾…ä»»åŠ¡é˜Ÿåˆ—æ¸…ç©º..."); job_q.join()
    logging.info("   - æ­£åœ¨å¼ºåˆ¶åˆ·æ–°æ‰€æœ‰æ‰¹å¤„ç†æ•°æ®åˆ°Excel..."); approved_batcher.flush(force=True)
    excel_writer_pool.shutdown(wait=True) # ç­‰å¾…Excelå†™å…¥ä»»åŠ¡å®Œæˆ
    logging.info("   - åˆ·æ–°å®Œæˆã€‚")
    logging.info("âœ… åå°ä»»åŠ¡å·²å®‰å…¨å¤„ç†å®Œæ¯•ï¼ŒæœåŠ¡é€€å‡ºã€‚")
    os._exit(0)

if __name__ == "__main__":
    consumer_thread = threading.Thread(target=consume_jobs, daemon=True); consumer_thread.start()
    atexit.register(graceful_shutdown)
    signal.signal(signal.SIGINT, graceful_shutdown); signal.signal(signal.SIGTERM, graceful_shutdown)
    load_fetched_list_to_redis()
    logging.info(f"ğŸš€ [v5.6 - åŸºäºv5.5ä¿®å¤ç‰ˆ] æœåŠ¡å¯åŠ¨ï¼šhttp://127.0.0.1:{FLASK_PORT}")
    logging.info(f"ğŸ“Š æ•°æ®é¢æ¿è¯·è®¿é—®: http://localhost:{FLASK_PORT}/dashboard")
    from waitress import serve
    serve(app, host="0.0.0.0", port=FLASK_PORT, threads=16)

# åˆ é™¤äº†æ‚¨ä»£ç æœ«å°¾é‡å¤å®šä¹‰çš„è·¯ç”±
