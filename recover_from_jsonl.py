# recover_from_jsonl.py - JSONLæ•°æ®æ¢å¤å·¥å…·
#
# ä½¿ç”¨æ–¹æ³•:
# 1. å°†æ­¤æ–‡ä»¶æ”¾ç½®åœ¨ä¸ xhs_user_spider_ai.py ç›¸åŒçš„é¡¹ç›®æ ¹ç›®å½•ä¸‹ã€‚
# 2. ç¡®ä¿ Redis æœåŠ¡æ­£åœ¨è¿è¡Œã€‚
# 3. ã€é‡è¦ã€‘å…³é—­æ‰€æœ‰å¯èƒ½æ­£åœ¨æ‰“å¼€çš„Excelæ–‡ä»¶ï¼ˆå¦‚ å·²é€šè¿‡æ•°æ®.xlsx ç­‰ï¼‰ã€‚
# 4. åœ¨ç»ˆç«¯ä¸­è¿è¡Œæ­¤è„šæœ¬: python recover_from_jsonl.py
# 5. è„šæœ¬ä¼šè‡ªåŠ¨è¯»å– mark_data.jsonl å¹¶å°†æœªå¤„ç†çš„æ•°æ®æ¢å¤åˆ°å¯¹åº”çš„Excelæ–‡ä»¶ä¸­ã€‚

import os
import io
import json
import hashlib
import threading
import time
from datetime import datetime
from typing import Optional, List, Dict
from concurrent.futures import ThreadPoolExecutor

import requests
from PIL import Image
import redis
from openpyxl import Workbook, load_workbook

# ========== 1. ä»ä¸»ç¨‹åºå¤åˆ¶è¿‡æ¥çš„åŸºç¡€é…ç½® (ä¿æŒå®Œå…¨ä¸€è‡´) ==========
APPROVED_EXCEL_PATH = "å·²é€šè¿‡æ•°æ®.xlsx"
# REJECTED_EXCEL_PATH = "æœªé€šè¿‡æ•°æ®.xlsx"
# NEW_TRAINING_DATA_EXCEL = "å¾…è®­ç»ƒæ•°æ®.xlsx"
# FETCHED_USER_LIST_PATH = "å·²çˆ¬å–ç”¨æˆ·å.xlsx" 
IMAGES_ROOT = os.path.join("data", "images")
IMAGE_MAX_SIDE = 768; IMAGE_FORMAT = "WEBP"; IMAGE_QUALITY = 90
REDIS_HOST = "localhost"; REDIS_PORT = 6379; REDIS_DB = 0
IMG_MAX_WORKERS  = 6; HTTP_TIMEOUT = (3, 8)
WAL_DIR   = os.path.join("data", "wal_final"); WAL_FILE  = os.path.join(WAL_DIR, "mark_data.jsonl")

# Redis Keys
SAVE_HISTORY_ENABLED_KEY = "save_history:enabled"
WAL_DONE_SET = "wal:done_final"

# é” (è™½ç„¶å•çº¿ç¨‹è¿è¡Œæ¢å¤è„šæœ¬ä¸æ˜¯å¿…é¡»ï¼Œä½†ä¿æŒå‡½æ•°ç­¾åä¸€è‡´æ€§)
fetched_user_list_lock = threading.Lock()
training_data_excel_lock = threading.Lock()
excel_write_lock = threading.Lock() # é€šç”¨Excelå†™é”

# ========== 2. ä»ä¸»ç¨‹åºå¤åˆ¶è¿‡æ¥çš„æ ¸å¿ƒå·¥å…·å‡½æ•° ==========
r = redis.Redis(host=REDIS_HOST, port=REDIS_PORT, db=REDIS_DB, decode_responses=True)
http = requests.Session()
img_pool = ThreadPoolExecutor(max_workers=IMG_MAX_WORKERS)
os.makedirs(IMAGES_ROOT, exist_ok=True)

def parse_count_chinese(s: str) -> int:
    s = str(s)
    if not s or s == "èµ": return 0
    trans = str.maketrans({**{chr(ord('ï¼')+i): str(i) for i in range(10)}, 'ï¼': '.', 'ï¼Œ': ','})
    s = s.translate(trans).replace(",", "").rstrip("+").strip()
    if s.endswith("ä¸‡"):
        try: return int(round(float(s[:-1]) * 10000))
        except: return 0
    try: return int(float(s))
    except: return 0

INFO_SHEET = "åšä¸»ä¿¡æ¯"; NOTES_SHEET = "åšä¸»ç¬”è®°"
INFO_BASE_COLS = ["ç”¨æˆ·å", "å°çº¢ä¹¦å·", "ä¸»é¡µç½‘å€", "é‚®ç®±", "æœç´¢è¯", "å®¡æ ¸çŠ¶æ€"]
INFO_AI_COLS = ["AIé¢„æµ‹æ¦‚ç‡", "AIé¢„æµ‹çŠ¶æ€"]
INFO_EXTRA_COLS = ["ä¸ªäººç®€ä»‹", "ç²‰ä¸æ•°", "æ€»ç‚¹èµ", "æ ‡è®°æ—¶é—´"]
ALL_INFO_COLS = INFO_BASE_COLS + INFO_AI_COLS + INFO_EXTRA_COLS

def ensure_workbook(xlsx_path: str):
    if not os.path.exists(xlsx_path): 
        wb = Workbook()
        ws_info = wb.active
        ws_info.title = INFO_SHEET
        ws_info.append(ALL_INFO_COLS)
        ws_notes = wb.create_sheet(NOTES_SHEET)
        ws_notes.append(["å°çº¢ä¹¦å·", "ç¬”è®°åºå·", "ç¬”è®°æ ‡é¢˜", "ç¬”è®°ç‚¹èµæ•°", "ç¬”è®°å°é¢è·¯å¾„", "æ ‡è®°æ—¶é—´"])
        wb.save(xlsx_path)

def excel_append_batch(xlsx_path: str, info_rows: List[Dict], notes_rows: List[List]):
    with excel_write_lock:
        ensure_workbook(xlsx_path)
        wb = load_workbook(xlsx_path)
        if info_rows: 
            ws = wb[INFO_SHEET]
            headers = [c.value for c in ws[1]]
            [ws.append([row_dict.get(h, "") for h in headers]) for row_dict in info_rows]
        if notes_rows: 
            ws = wb[NOTES_SHEET]
            [ws.append(row) for row in notes_rows]
        wb.save(xlsx_path)

def download_and_convert_image(url: str, userid: str) -> str:
    if not url or not userid: return ""
    try:
        resp = http.get(url, timeout=HTTP_TIMEOUT, stream=True)
        if resp.status_code != 200: return ""
        raw = resp.content; h = hashlib.sha1(raw).hexdigest(); subdir = os.path.join(IMAGES_ROOT, userid, h[:2]); os.makedirs(subdir, exist_ok=True)
        out_path = os.path.join(subdir, f"{h}.webp")
        if os.path.exists(out_path): return os.path.relpath(out_path).replace("\\", "/")
        im = Image.open(io.BytesIO(raw)).convert("RGB"); w, hgt = im.size
        if max(w, hgt) > IMAGE_MAX_SIDE: ratio = IMAGE_MAX_SIDE / max(w, hgt); im = im.resize((int(w * ratio), int(hgt * ratio)), Image.LANCZOS)
        im.save(out_path, format=IMAGE_FORMAT, quality=IMAGE_QUALITY, method=6)
        return os.path.relpath(out_path).replace("\\", "/")
    except Exception: return ""

# ========== 3. ä»ä¸»ç¨‹åºå¤åˆ¶è¿‡æ¥çš„æ•°æ®å¤„ç†é€»è¾‘å‡½æ•° ==========
def append_to_fetched_list(username: Optional[str], userid: Optional[str]):
    if not username and not userid: return
    with fetched_user_list_lock:
        try:
            sheet_name_user, sheet_name_id = "çˆ¬å–ç”¨æˆ·å", "å°çº¢ä¹¦å·"
            if not os.path.exists(FETCHED_USER_LIST_PATH): wb = Workbook(); ws_user = wb.active; ws_user.title = sheet_name_user; ws_user.append([sheet_name_user]); ws_id = wb.create_sheet(sheet_name_id); ws_id.append([sheet_name_id]); wb.save(FETCHED_USER_LIST_PATH)
            wb = load_workbook(FETCHED_USER_LIST_PATH)
            if username: ws_user = wb[sheet_name_user] if sheet_name_user in wb.sheetnames else wb.create_sheet(sheet_name_user, 0); ws_user.append([username])
            if userid: ws_id = wb[sheet_name_id] if sheet_name_id in wb.sheetnames else wb.create_sheet(sheet_name_id, 1); ws_id.append([str(userid)])
            wb.save(FETCHED_USER_LIST_PATH)
        except Exception as e: print(f"âŒ (æ¢å¤)è¿½åŠ åˆ° '{FETCHED_USER_LIST_PATH}' å¤±è´¥: {e}")

def append_to_training_excel(job: Dict):
    if not job or not job.get("status"): return
    with training_data_excel_lock:
        try:
            info_row, notes_rows = build_excel_rows(job)
            excel_append_batch(NEW_TRAINING_DATA_EXCEL, [info_row], notes_rows)
        except Exception as e:
            print(f"âŒ (æ¢å¤)å†™å…¥è®­ç»ƒæ•°æ®åˆ° '{NEW_TRAINING_DATA_EXCEL}' å¤±è´¥: {e}")

def build_excel_rows(job: Dict):
    info_row_map = {
        "ç”¨æˆ·å": "username", "å°çº¢ä¹¦å·": "userid", "ä¸»é¡µç½‘å€": "url", "é‚®ç®±": "email", "æœç´¢è¯": "search_term", 
        "å®¡æ ¸çŠ¶æ€": "status", "AIé¢„æµ‹æ¦‚ç‡": "ai_prob", "AIé¢„æµ‹çŠ¶æ€": "ai_decision", "ä¸ªäººç®€ä»‹": "bio", "ç²‰ä¸æ•°": "followers", 
        "æ€»ç‚¹èµ": "likes_total", "æ ‡è®°æ—¶é—´": "timestamp"
    }
    info_row = {k: job.get(v, "") for k, v in info_row_map.items()}
    notes_rows = []; userid = job.get("userid") or job.get("username") or "unknown"
    futures = {i: img_pool.submit(download_and_convert_image, n.get("cover_url", ""), userid) for i, n in enumerate(job.get("notes", [])[:20]) if n.get("cover_url")}
    covers = {i: fut.result() for i, fut in futures.items()}
    for i, n in enumerate(job.get("notes", [])[:20]): notes_rows.append([job.get("userid", ""), i + 1, n.get("title", ""), parse_count_chinese(str(n.get("likes", "0"))), covers.get(i, ""), job.get("timestamp", "")])
    return info_row, notes_rows

# ========== 4. æ¢å¤è„šæœ¬çš„ä¸»é€»è¾‘ ==========
if __name__ == "__main__":
    print("--- å¼€å§‹ä» mark_data.jsonl æ¢å¤æ•°æ®åˆ° Excel ---")
    print("â„¹ï¸ è¿™ä¸ªè„šæœ¬ä¸éœ€è¦åŠ è½½åºå¤§çš„AIæ¨¡å‹ï¼Œåªè¿›è¡Œæ•°æ®å¤„ç†ã€‚")
    
    if not os.path.exists(WAL_FILE):
        print(f"âŒ æœªæ‰¾åˆ°æ—¥å¿—æ–‡ä»¶ '{WAL_FILE}'ï¼Œæ— éœ€æ¢å¤ã€‚")
        exit()
        
    try:
        r.ping()
        print("âœ… Redis è¿æ¥æˆåŠŸã€‚")
    except redis.exceptions.ConnectionError as e:
        print(f"âŒ Redis è¿æ¥å¤±è´¥: {e}")
        print("   è¯·ç¡®ä¿æ‚¨çš„RedisæœåŠ¡æ­£åœ¨ localhost:6379 ä¸Šè¿è¡Œã€‚")
        exit()

    total_lines = 0
    processed_count = 0
    skipped_count = 0
    error_count = 0
    
    # è¯»å–æ‰€æœ‰jobåˆ°å†…å­˜ï¼Œæ–¹ä¾¿æ˜¾ç¤ºè¿›åº¦
    with open(WAL_FILE, 'r', encoding='utf-8') as f:
        jobs_to_process = f.readlines()
    
    total_lines = len(jobs_to_process)
    print(f"ğŸ“„ æ—¥å¿—æ–‡ä»¶ä¸­å…±æ‰¾åˆ° {total_lines} æ¡è®°å½•ã€‚")

    for i, line in enumerate(jobs_to_process):
        line = line.strip()
        if not line:
            continue

        progress = f"[{i+1}/{total_lines}]"

        try:
            job = json.loads(line)
        except json.JSONDecodeError:
            print(f"âš ï¸ {progress} è­¦å‘Š: è§£æJSONå¤±è´¥ï¼Œè·³è¿‡æŸåçš„è¡Œã€‚")
            error_count += 1
            continue

        job_id = job.get("job_id")
        if not job_id:
            print(f"âš ï¸ {progress} è­¦å‘Š: è®°å½•ç¼ºå°‘ 'job_id'ï¼Œè·³è¿‡ã€‚")
            error_count += 1
            continue

        # æ ¸å¿ƒé€»è¾‘ï¼šæ£€æŸ¥æ˜¯å¦å·²å¤„ç†è¿‡
        if r.sismember(WAL_DONE_SET, job_id):
            # print(f"â­ï¸ {progress} å·²è·³è¿‡ (å·²å¤„ç†): {job_id}") # æ—¥å¿—å¤ªé•¿ï¼Œé»˜è®¤ä¸æ‰“å°
            skipped_count += 1
            continue
        
        print(f"âš™ï¸ {progress} æ­£åœ¨å¤„ç†æ–°ä»»åŠ¡: {job.get('username') or job.get('userid')} ({job.get('status')})")
        
        try:
            # 1. å†™å…¥è®­ç»ƒæ•°æ® (åŒ…å«å›¾ç‰‡ä¸‹è½½)
            append_to_training_excel(job)
            
            # 2. è¿½åŠ åˆ°å·²çˆ¬å–åˆ—è¡¨
            append_to_fetched_list(job.get("username"), job.get("userid"))
            
            # 3. å†™å…¥ä¸»æ•°æ®æ–‡ä»¶ (å·²é€šè¿‡/æœªé€šè¿‡)
            save_history_enabled = r.get(SAVE_HISTORY_ENABLED_KEY) == "1"
            status = job.get("status")
            
            if status == "ç¬¦åˆ" or (status == "ä¸ç¬¦åˆ" and save_history_enabled):
                # build_excel_rows å·²ç»åœ¨ append_to_training_excel ä¸­è¢«è°ƒç”¨è¿‡ä¸€æ¬¡
                # ä¸ºäº†æ•ˆç‡ï¼Œæˆ‘ä»¬ç›´æ¥å¤ç”¨é‚£ä¸ªé€»è¾‘ï¼Œæˆ–è€…å†è°ƒç”¨ä¸€æ¬¡ä»¥ç¡®ä¿æ•°æ®éš”ç¦»
                info_row, notes_rows = build_excel_rows(job)
                
                xlsx_path = APPROVED_EXCEL_PATH if status == "ç¬¦åˆ" else REJECTED_EXCEL_PATH
                excel_append_batch(xlsx_path, [info_row], notes_rows)
            
            # 4. æ ‡è®°ä»»åŠ¡å·²å®Œæˆ
            r.sadd(WAL_DONE_SET, job_id)
            processed_count += 1

        except Exception as e:
            print(f"âŒ {progress} å¤„ç†ä»»åŠ¡ {job_id} æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
            error_count += 1
            
    print("\n" + "="*40)
    print("âœ… æ•°æ®æ¢å¤å®Œæˆï¼")
    print("="*40)
    print(f"ğŸ“„ æ€»è®°å½•æ•°: {total_lines}")
    print(f"âš™ï¸ æˆåŠŸå¤„ç†: {processed_count} æ¡")
    print(f"â­ï¸ è·³è¿‡ (å·²å­˜åœ¨): {skipped_count} æ¡")
    print(f"âŒ å¤±è´¥/é”™è¯¯: {error_count} æ¡")
    print("\nç°åœ¨æ‚¨å¯ä»¥æ£€æŸ¥æ‚¨çš„Excelæ–‡ä»¶äº†ã€‚")

